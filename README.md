# ML From Scratch
 Attempting to implement Machine Learning algorithms and models from scratch without any ML/DL frameworks. (Using only Numpy)

 I first created a two, three, and five layer net from scratch, implementing a basic forward and backward pass for each. 
    - These exercises was to ensure I knew the basic mechanics of forward and backward propagation through a network.

I then created a N layer net, which can create a network with an arbitrary number of layers. I also added batch normalization and dropout.

These exercises is a compliment to Stanford's cs231n course, which is a course about Image Recognition. The purpose of doing these exercises is to practice programming neural networks on my own, without the use of PyTorch, TensorFlow, etc. I can gain an understanding of th internal mechanism that allow for these nets to function, so that when I finally dive deep into these libraries, I can understand what I am using.

Code isn't great, and no where near 'elegant', but steadily improving in that regard.
TO DO: 
    - Completion of Batch Norm.
    - Addition of Dropout.
    - Improving the training/testing of the NN.



