{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple implementation of a three layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Initialization of the network paratmeters input X, and labels y.\n",
    "np.random.seed(231)\n",
    "N, D, H1, H2, C = 50, 3072, 100, 100, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "# Initialization of the layer parameters weights and biases.\n",
    "weight_scale = 5e-2\n",
    "W1 = weight_scale * np.random.randn(D, H1)\n",
    "b1 = weight_scale * np.zeros(H1)\n",
    "W2 = weight_scale * np.random.randn(H1, H2)\n",
    "b2  = weight_scale * np.zeros(H2)\n",
    "W3 = weight_scale * np.random.randn(H2, C)\n",
    "b3 = weight_scale * np.zeros(C)\n",
    "\n",
    "# Network architecture: (affine - relu) - (affine - relu) - (affine - softmax)\n",
    "class ThreeLayerNet:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def affine_forward(self, x, w, b):\n",
    "        out = x.reshape(x.shape[0], -1).dot(w) + b\n",
    "        cache = (x, w, b)\n",
    "        return out, cache\n",
    "    \n",
    "    def affine_backward(self, dout, cache):\n",
    "        x, w, b = cache\n",
    "        dx = dout.dot(w.T)\n",
    "        dw = x.T.dot(dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        return dx, dw, db\n",
    "    \n",
    "    def relu_forward(self, x):\n",
    "        out = np.maximum(0, x)\n",
    "        cache = x\n",
    "        return out, cache\n",
    "    \n",
    "    def relu_backward(self, dout, cache):\n",
    "        x = cache\n",
    "        return dout * (x > 0)\n",
    "    \n",
    "    def softmax(self, x, y):\n",
    "        exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        softmax = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        correct_scores = np.zeros(x.shape)\n",
    "        correct_scores[range(len(y)), y] = 1\n",
    "        loss = np.mean(-np.log(np.sum(softmax * correct_scores, axis=1, keepdims=True)))\n",
    "        dL = (softmax - correct_scores) / len(softmax)\n",
    "        return loss, dL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 2.421977830511722\n",
      "the shape of the derivative of the loss wrt. the scores (H3): (50, 10)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass.\n",
    "nn = ThreeLayerNet()\n",
    "H1, cache_H1 = nn.affine_forward(X, W1, b1)\n",
    "R1, cache_R1 = nn.relu_forward(H1)\n",
    "H2, cache_H2 = nn.affine_forward(R1, W2, b2)\n",
    "R2, cache_R2 = nn.relu_forward(H2)\n",
    "H3, cache_H3 = nn.affine_forward(R2, W3, b3)\n",
    "loss, dL = nn.softmax(H3, y)\n",
    "print(f'loss: {loss}')\n",
    "print(f'the shape of the derivative of the loss wrt. the scores (H3): {dL.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the gradients of W3: (100, 10)\n",
      "the shape of the gradients of b3: (10,)\n",
      "the shape of the gradients of the second relu function: (50, 100)\n",
      "the shape of the gradients of W2: (100, 100)\n",
      "the shape of the gradients of b2: (100,)\n",
      "the shape of the gradients of the first relu function: (50, 100)\n",
      "the shape of the gradients of W1: (3072, 100)\n",
      "the shape of the gradients of b2: (100,)\n"
     ]
    }
   ],
   "source": [
    "# Backward pass.\n",
    "grads = {}\n",
    "dout3, grads['W3'], grads['b3'] = nn.affine_backward(dL, cache_H3)\n",
    "print(f'the shape of the gradients of W3:', grads['W3'].shape)\n",
    "print(f'the shape of the gradients of b3:', grads['b3'].shape)\n",
    "\n",
    "drelu2 = nn.relu_backward(dout3, cache_R2)\n",
    "print(f'the shape of the gradients of the second relu function:', drelu2.shape)\n",
    "\n",
    "dout2, grads['W2'], grads['b2'] = nn.affine_backward(drelu2, cache_H2)\n",
    "print(f'the shape of the gradients of W2:', grads['W2'].shape)\n",
    "print(f'the shape of the gradients of b2:', grads['b2'].shape)\n",
    "\n",
    "drelu1 = nn.relu_backward(dout2, cache_R1)\n",
    "print(f'the shape of the gradients of the first relu function:', drelu1.shape)\n",
    "\n",
    "dout1, grads['W1'], grads['b1'] = nn.affine_backward(drelu1, cache_H1)\n",
    "print(f'the shape of the gradients of W1:', grads['W1'].shape)\n",
    "print(f'the shape of the gradients of b2:', grads['b1'].shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
