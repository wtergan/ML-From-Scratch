{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple implementation of a five layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 (3072, 100)\n",
      "b1 (100,)\n",
      "W2 (100, 100)\n",
      "b2 (100,)\n",
      "W3 (100, 100)\n",
      "b3 (100,)\n",
      "W4 (100, 100)\n",
      "b4 (100,)\n",
      "W5 (100, 10)\n",
      "b5 (10,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Initialization of the network parameters input X, and labels y.\n",
    "np.random.seed(231)\n",
    "H_dims = [100, 100, 100, 100]\n",
    "N, D, C = 50, 3 * 32 * 32, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "weight_scale = 5e-2\n",
    "num_layers = len(H_dims) + 1\n",
    "model_params = {}\n",
    "\n",
    "# Weight and Biases initialization. Can handle an arbitrary number of hidden dimensions. \n",
    "for layer, hidden_dims in enumerate(H_dims):\n",
    "    if layer == 0:\n",
    "        model_params['W' + str(layer+1)] = weight_scale * np.random.randn(D, hidden_dims)\n",
    "        model_params['b' + str(layer+1)] = np.zeros(hidden_dims)\n",
    "    else:\n",
    "        model_params['W' + str(layer+1)] = weight_scale * np.random.randn(hidden_dims, hidden_dims)\n",
    "        model_params['b' + str(layer+1)] = np.zeros(hidden_dims)\n",
    "model_params['W' + str(len(H_dims) + 1)] = weight_scale * np.random.randn(H_dims[-1], C)\n",
    "model_params['b' + str(len(H_dims) + 1)] = np.zeros(C)\n",
    "\n",
    "for p in model_params.keys():\n",
    "    print(p, model_params[p].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network architecture: In this case: Five Layer Net.\n",
    "#(affine-relu) -> (affine-relu) -> (affine-relu) -> (affine-relu) -> (affine-softmax).\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def affine_forward(self, x, w, b):\n",
    "        out = x.reshape(x.shape[0], -1).dot(w) + b\n",
    "        cache = (x, w, b)\n",
    "        return out, cache\n",
    "    \n",
    "    def affine_backward(self, dout, cache):\n",
    "        x, w, b = cache\n",
    "        dx = dout.dot(w.T)\n",
    "        dw = x.T.dot(dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        return dx, dw, db\n",
    "\n",
    "    def relu_forward(self, x):\n",
    "        out = np.maximum(0, x)\n",
    "        cache = x\n",
    "        return out, cache\n",
    "    \n",
    "    def relu_backward(self, dout, cache):\n",
    "        x = cache\n",
    "        return dout * (x > 0)\n",
    "    \n",
    "    def softmax(self, x, y):\n",
    "        exp_values = np.exp(x - np.max(x, axis=1, keepdims = True))\n",
    "        softmax = exp_values / exp_values.sum(axis=1, keepdims=True)\n",
    "        correct_scores = np.zeros(x.shape)\n",
    "        correct_scores[range(len(y)), y] = 1\n",
    "        loss = np.mean(-np.log(np.sum(softmax * correct_scores, axis=1, keepdims=True)))\n",
    "        dL = (softmax - correct_scores) / len(softmax)\n",
    "        return loss, dL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss : 2.301195972715804\n",
      "the shape of the gradients of the loss wrt. the scores (H5): (50, 10)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass:\n",
    "W1, b1, W2, b2, W3, b3, W4, b4, W5, b5 = model_params.values()\n",
    "\n",
    "nn = NeuralNet()\n",
    "\n",
    "H1, cache_H1 = nn.affine_forward(X, W1, b1)\n",
    "R1, cache_R1 = nn.relu_forward(H1)\n",
    "H2, cache_H2 = nn.affine_forward(R1, W2, b2)\n",
    "R2, cache_R2 = nn.relu_forward(H2)\n",
    "H3, cache_H3 = nn.affine_forward(R2, W3, b3)\n",
    "R3, cache_R3 = nn.relu_forward(H3)\n",
    "H4, cache_H4 = nn.affine_forward(R3, W4, b4)\n",
    "R4, cache_R4 = nn.relu_forward(H4)\n",
    "H5, cache_H5 = nn.affine_forward(R4, W5, b5)\n",
    "\n",
    "loss, dL = nn.softmax(H5, y)\n",
    "print(f'loss : {loss}')\n",
    "print(f'the shape of the gradients of the loss wrt. the scores (H5): {dL.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the gradients of W5 and b5 respectively:  (100, 10) (10,)\n",
      "the shape of the gradients of dout5, which is used to backprop through relu/previous layer: (50, 100)\n",
      "the shape of the gradients of W4 and b4 respectively:  (100, 100) (100,)\n",
      "the shape of the gradients of dout4, which is used to backprop through relu/previous layer: (50, 100)\n",
      "the shape of the gradients of W3 and b3 respectively:  (100, 100) (100,)\n",
      "the shape of the gradients of dout3, which is used to backprop through relu/previous layer: (50, 100)\n",
      "the shape of the gradients of W2 and b2 respectively:  (100, 100) (100,)\n",
      "the shape of the gradients of dout2, which is used to backprop through relu/previous layer: (50, 100)\n",
      "the shape of the gradients of W1 and b1 respectively:  (3072, 100) (100,)\n",
      "the shape of the gradients of dout1, which is just the gradients of the original input X: (50, 3072)\n"
     ]
    }
   ],
   "source": [
    "'''Backward pass:\n",
    "    We will compute the gradients of the weights and biases for each layer, as well as the gradient of x (dout).\n",
    "    We will do the chain rule of the dout with the derivatve of the activation function (relu).\n",
    "    This result is used to backpropagate the gradients to the precceeding layer.\n",
    "    When we reach the end layer (first layer), the gradient of x will not have to be multiplied by anything else.\n",
    "     as this is the end of the network.\n",
    "'''\n",
    "\n",
    "grads = {}\n",
    "# Gradients of the 5th layer: (W5, b5, and X5, which is R4).\n",
    "dout5, grads['W5'], grads['b5'] = nn.affine_backward(dL, cache_H5)\n",
    "print(f'the shape of the gradients of W5 and b5 respectively: ', grads['W5'].shape, grads['b5'].shape)\n",
    "print(f'the shape of the gradients of dout5, which is used to backprop through relu/previous layer:', dout5.shape)\n",
    "\n",
    "# The gradient of the 4th relu activation function. \n",
    "drelu4 = nn.relu_backward(dout5, cache_R4)\n",
    "\n",
    "# Gradients of the 4th layer: (W4, b4, and X4, which is R3).\n",
    "dout4, grads['W4'], grads['b4'] = nn.affine_backward(drelu4, cache_H4)\n",
    "print(f'the shape of the gradients of W4 and b4 respectively: ', grads['W4'].shape, grads['b4'].shape)\n",
    "print(f'the shape of the gradients of dout4, which is used to backprop through relu/previous layer:', dout4.shape)\n",
    "\n",
    "# The gradient of the 3th relu activation function. \n",
    "drelu3 = nn.relu_backward(dout4, cache_R3)\n",
    "\n",
    "# Gradients of the 3rd layer: (W3, b3, and X3, which is R2).\n",
    "dout3, grads['W3'], grads['b3'] = nn.affine_backward(drelu3, cache_H3)\n",
    "print(f'the shape of the gradients of W3 and b3 respectively: ', grads['W3'].shape, grads['b3'].shape)\n",
    "print(f'the shape of the gradients of dout3, which is used to backprop through relu/previous layer:', dout3.shape)\n",
    "\n",
    "# The gradient of the 2th relu activation function. \n",
    "drelu2 = nn.relu_backward(dout3, cache_R2)\n",
    "\n",
    "# Gradients of the 2th layer: (W2, b2, and X2, which is R1).\n",
    "dout2, grads['W2'], grads['b2'] = nn.affine_backward(drelu2, cache_H2)\n",
    "print(f'the shape of the gradients of W2 and b2 respectively: ', grads['W2'].shape, grads['b2'].shape)\n",
    "print(f'the shape of the gradients of dout2, which is used to backprop through relu/previous layer:', dout2.shape)\n",
    "\n",
    "# The gradient of the 1st relu activation function. \n",
    "drelu1 = nn.relu_backward(dout2, cache_R1)\n",
    "\n",
    "# Gradients of the 1st layer: (W1, b1, and X1, which is R1).\n",
    "dout1, grads['W1'], grads['b1'] = nn.affine_backward(drelu1, cache_H1)\n",
    "print(f'the shape of the gradients of W1 and b1 respectively: ', grads['W1'].shape, grads['b1'].shape)\n",
    "print(f'the shape of the gradients of dout1, which is just the gradients of the original input X:', dout1.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
