{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple implementation of a two layer neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Initialization of the network parameters, input X and labels y.\n",
    "np.random.seed(231)\n",
    "N, D, H1, C = 3, 5, 50, 7\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "# Initialization of the layer parameters weights and biases.\n",
    "weight_scale = 5e-2\n",
    "W1 = weight_scale * np.random.randn(D, H1)\n",
    "b1 = weight_scale * np.zeros(H1)\n",
    "W2 = weight_scale * np.random.randn(H1, C)\n",
    "b2 = weight_scale * np.zeros(C)\n",
    "\n",
    "# Network's architecture: (affine - relu) - (affine - softmax).\n",
    "class TwoLayerNet:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def affine_forward(self, x, w, b): \n",
    "        out = x.reshape(x.shape[0], -1).dot(w) + b\n",
    "        cache = (x, w, b)\n",
    "        return out, cache\n",
    "    \n",
    "    def affine_backward(self, dout, cache):\n",
    "        x, w, b = cache\n",
    "        dw = x.T.dot(dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        return dw, b\n",
    "\n",
    "    def relu_forward(self, x):\n",
    "        out =  np.maximum(0, x)\n",
    "        cache = (x)\n",
    "        return out, cache\n",
    "\n",
    "    def relu_backward(self, dout, cache):\n",
    "        x = cache\n",
    "        # (3, 50) * ((3,7)*(7,50))\n",
    "        return np.where(x > 0, 1, 0) * dout.dot(W2.T)\n",
    "\n",
    "    def softmax_loss(self, x, y):\n",
    "        exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        softmax = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        correct_scores = np.zeros(x.shape)\n",
    "        correct_scores[range(len(y)), y] = 1\n",
    "        loss = np.mean(-np.log(np.sum(softmax * correct_scores, axis=1, keepdims=True)))\n",
    "        dL = (softmax - correct_scores) / len(softmax)\n",
    "        return loss, dL\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 1.930094367227203\n",
      "shape of the derivative of the loss wrt. the softmax values (layer_2): (3, 7)\n"
     ]
    }
   ],
   "source": [
    "# Forward pass.\n",
    "nn = TwoLayerNet()\n",
    "layer_1, cache_l1 = nn.affine_forward(X, W1, b1)\n",
    "relu, cache_relu = nn.relu_forward(layer_1)\n",
    "layer_2, cache_l2 = nn.affine_forward(relu, W2, b2)\n",
    "loss, dL = nn.softmax_loss(layer_2, y)\n",
    "print(f'loss: {loss}')\n",
    "print(f'shape of the derivative of the loss wrt. the softmax values (layer_2): {dL.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the shape of the gradients of W2: (50, 7)\n",
      "the shape of the gradients of b2: (7,)\n",
      "the shape of the gradients of relu: (3, 50)\n",
      "the shape of the gradients of W1: (5, 50)\n",
      "the shape of the gradients of b1: (50,)\n"
     ]
    }
   ],
   "source": [
    "# Backward pass.\n",
    "grads = {}\n",
    "grads['W2'], grads['b2'] = nn.affine_backward(dL, cache_l2)\n",
    "print('the shape of the gradients of W2:', grads['W2'].shape)\n",
    "print('the shape of the gradients of b2:', grads['b2'].shape)\n",
    "\n",
    "drelu = nn.relu_backward(dL, cache_relu)\n",
    "print(f'the shape of the gradients of relu:', drelu.shape)\n",
    "\n",
    "grads['W1'], grads['b1'] = nn.affine_backward(drelu, cache_l1)\n",
    "print(f'the shape of the gradients of W1:', grads['W1'].shape)\n",
    "print(f'the shape of the gradients of b1:', grads['b1'].shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0eb7a405789a2ad0bd2595b62391548e17e9d0e8722ef2b8a9281ed387d58286"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
