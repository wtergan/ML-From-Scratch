{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple implementation of an neural network of N size:\n",
    "    - where N is the number of layers that constitute the network.\n",
    "\n",
    "Unlike previous iterations of the net we have created, for this one we would have to modularize the initialization of the network's parameters, as well as make the forward and backward passes that handles those N layers.\n",
    "\n",
    "Also, unlike previous iterations of the net we have created, this one will include 'batch normalization', which is used to maintain a fixed distibution of the values of each layer of the network as the network trains. This mitigates 'Internal Covariate Shift', which is the phenomenon that describes the change in the distribution of the network activations due to change in network parameters.\n",
    "\n",
    "We will also implement dropout, which drops random neurons in each layer during training, so as to help mitigate overfitting, as well as allow neurons which would not commonly be used for training to be used more readily, so as to reduce co-adaptation of the neurons.\n",
    "\n",
    "We will ask the user for the number of total layers for the network to be created, as well as the size of each of its constituent hidden layers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assist in helping one to follow the code, for our user inputs for the number of layers and the size of each of the hidden layers, we will default to:\n",
    "\n",
    "            num_layers = 5\n",
    "            H_dims = size = 100, for each layer.\n",
    "\n",
    "One can adjust, but it will be easier to follow the code with these values in mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Initialization of the network parameters input X, and the labels y, as well as hidden dims.\n",
    "np.random.seed(231)\n",
    "num_layers = int(input('how many layers do you want in your network?'))\n",
    "H_dims = [int(input('size of layer'+str(x))) for x in range(1, num_layers)]\n",
    "N, D, C = 50, 3*32*32, 10\n",
    "X = np.random.randn(N, D)\n",
    "y = np.random.randint(C, size=(N,))\n",
    "\n",
    "# Some defualt hyperparameters to be used.\n",
    "weight_scale = 5e-2\n",
    "eps = 1e-5\n",
    "reg_strength = 0.01\n",
    "learning_rate = 0.2\n",
    "momentum = 0.9\n",
    "model_params = {}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N layer NeuralNet class implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network architecture: \n",
    "# (affine-batchnorm-relu) * num_layers-1 -> (affine-softmax)\n",
    "\n",
    "class NeuralNet:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def affine_forward(self, x, w, b):\n",
    "        # Input x's second dimension is reshaped to reflect D dimensional vector.\n",
    "        out = x.reshape(x.shape[0], -1).dot(w) + b\n",
    "        cache = (x, w, b)\n",
    "        return out, cache\n",
    "    \n",
    "    def affine_backward(self, dout, cache):\n",
    "        # Basic derivative/chain rule rules being applied here.\n",
    "        x, w, b = cache\n",
    "        dx = dout.dot(w.T)\n",
    "        dw = x.T.dot(dout)\n",
    "        db = np.sum(dout, axis=0)\n",
    "        return dx, dw, db\n",
    "    \n",
    "    def relu_forward(self, x):\n",
    "        # If value in x is > 0, return x, otherwise 0.\n",
    "        out = np.maximum(0, x)\n",
    "        cache = x\n",
    "        return out, cache\n",
    "    \n",
    "    def relu_backward(self, dout, cache):\n",
    "        # Since derivatiive of relu is 1 if x > 0, just return dout is x > 0.\n",
    "        x = cache\n",
    "        return dout * (x > 0)\n",
    "    \n",
    "    def batchnorm_forward(self, x, gamma, beta, eps):\n",
    "        # Do the normalization of input x: batch mean, variance, std.\n",
    "        b_mean = np.mean(x, axis=0)\n",
    "        b_var = np.var(x, axis=0)\n",
    "        b_std = np.sqrt(b_var + eps)\n",
    "        x_norm = (x - b_mean) / b_std\n",
    "\n",
    "        # Applying linear transformation on the normalized x. (gamma and beta):\n",
    "        out = (gamma * x_norm) + beta\n",
    "        cache = (x, x_norm, gamma, beta, b_mean, b_var, b_std, eps)\n",
    "        return out, cache\n",
    "    \n",
    "    def batchnorm_backward(self, dout, cache):\n",
    "        # Computation of the gradients of x, gamma, and beta.\n",
    "        x, x_norm, gamma, beta, b_mean, b_var, b_std, eps = cache\n",
    "        dbeta = np.sum(dout, axis=0)                                            # derivative of beta wrt. dout. \n",
    "        dgamma = np.sum(x_norm * gamma, axis=0)                                 # derivative of gamma wrt. dout.\n",
    "        dx_norm =  dout * gamma                                                 # derivative of x_norm wrt. dout (mult. rule)\n",
    "        d_std = -np.sum(dx_norm * (x - b_mean), axis=0) / (b_std**2)            # derivative of std.\n",
    "        d_variance = 0.5 *  d_std / b_std                                       # derivative of variance.\n",
    "        dx1  = dx_norm / b_std + 2 * (x - b_mean) * d_variance / len(dout)      # derivative of dx1. \n",
    "        d_mean = -np.sum(dx1, axis=0)                                           # derivative of the mean.\n",
    "        dx2 = d_mean / len(dout)                                                # derivative of dx2.\n",
    "        dx = dx1 + dx2\n",
    "        return dx, dgamma, dbeta\n",
    "    \n",
    "    def print_mean_std(self, x):\n",
    "        # Take the value from the batchnorm forward pass, and calculate the mean and std.\n",
    "        # Mean should be close to 0, std should be close to 1.\n",
    "        # This is used to check if batchnorm_forward is implemented correctly.\n",
    "        mean = np.mean(x, axis=0)\n",
    "        std = np.sqrt(np.var(x, axis=0) + eps)\n",
    "        return mean, std\n",
    "\n",
    "    def softmax(self, x, y):\n",
    "        # Computation of the softmax, used to compute the loss and gradients of the loss wrt. the scores.\n",
    "        exp_values = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "        softmax = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
    "        correct_scores = np.zeros(x.shape)\n",
    "        correct_scores[range(len(y)), y] = 1\n",
    "        loss = np.mean(-np.log(np.sum(softmax * correct_scores, axis=1, keepdims=True)))\n",
    "        dL = (softmax - correct_scores) / len(softmax)\n",
    "        return loss, dL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, lets implement a class that will comprise of the forward pass and backward passes. Then, It will optimize the gradients by applying one of the various optimizers (sgd, momentum, etc.) to update the network's parameters. This is what makes the network learn!\n",
    "\n",
    "Following the network's architechure, we will:\n",
    "\n",
    "    -compute the affine, batch, and relu passes. \n",
    "\n",
    "    -then at the end we will do affine (to compute scores) to softmax (to compute loss). \n",
    "    \n",
    "    -once loss and gradients of the loss wrt. scores is computed, backward propogate through the network. \n",
    "    \n",
    "    -once you reach end of backprop, update the parameters (using one of various optimizers). \n",
    "    \n",
    "    -then forward prop again. Continue until loss decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For simplicity, lets make a new class called 'Solver', which will constitute the forward and backward passes.\n",
    "\n",
    "class Solver:\n",
    "    def __init__(self, X, y, N, D, num_layers, H_dims, learning_rate, weight_scale, reg_strength):\n",
    "        # Learning rate. We can try different rates to see which performs best.\n",
    "        pass\n",
    "\n",
    "    def param_init(self):\n",
    "        # Weight and Biases initialization. Can handle an arbitrary number of hidden dimensions.\n",
    "        # Also, addition of gamma and beta parameters, which are used for the batch normalization .\n",
    "        for layer, hidden_dims in enumerate(H_dims):\n",
    "            if layer == 0:\n",
    "                model_params['W'+str(layer+1)] = weight_scale * np.random.randn(D, hidden_dims)\n",
    "                model_params['b'+str(layer+1)] = np.zeros(hidden_dims)\n",
    "                model_params['gamma'+str(layer+1)] = np.ones(hidden_dims)\n",
    "                model_params['beta'+str(layer+1)] = np.zeros(hidden_dims)\n",
    "            else:\n",
    "                model_params['W'+str(layer+1)] = weight_scale * np.random.randn(H_dims[layer-1], hidden_dims)\n",
    "                model_params['b'+str(layer+1)] = np.zeros(hidden_dims)\n",
    "                model_params['gamma'+str(layer+1)] = np.ones(hidden_dims)\n",
    "                model_params['beta'+str(layer+1)] = np.zeros(hidden_dims)\n",
    "        model_params['W'+str(num_layers)] = weight_scale * np.random.randn(H_dims[-1], C)\n",
    "        model_params['b'+str(num_layers)] = np.zeros(C)\n",
    "        return model_params\n",
    "\n",
    "    def forward_pass(self, model_params):\n",
    "        # Forward pass.\n",
    "        nn = NeuralNet()\n",
    "        # Lists that holds cache values of each part of the layers. (affine, batch, and relu caches). Used for backprop.\n",
    "        layer_out = []\n",
    "        affine_cache = []\n",
    "        batch_cache = []\n",
    "        relu_cache = []\n",
    "\n",
    "        # Loop that computes the forward pass an arbitrary amount of times (number of layers)\n",
    "        for layer in range(num_layers - 1):\n",
    "            # If the first layer, use input X in the affine_forward() method.\n",
    "            if layer == 0:\n",
    "                w, b = model_params['W'+str(layer+1)], model_params['b'+str(layer+1)]\n",
    "                a_values, a_cache_values = nn.affine_forward(X, w, b)\n",
    "                \n",
    "            # Otherwise, input x is the value of the relu activation function, which is in the layer_out list.\n",
    "            else:\n",
    "                x, w, b = layer_out[layer-1], model_params['W'+str(layer+1)], model_params['b'+str(layer+1)]\n",
    "                a_values, a_cache_values = nn.affine_forward(x, w, b)\n",
    "\n",
    "            # Apply batchnorm to the affine forward output.\n",
    "            gamma, beta = model_params['gamma'+str(layer+1)], model_params['beta'+str(layer+1)]\n",
    "            b_values, b_cache_values = nn.batchnorm_forward(a_values, gamma, beta, eps)\n",
    "\n",
    "            # Compute the relu activation function using output from batchnorm. Append relu output in layer_out list.\n",
    "            r_values, r_cache_values = nn.relu_forward(b_values)\n",
    "            layer_out.append(r_values)\n",
    "            \n",
    "            # Append the cache values from the affine, batch, and relu passes into its respective cache lists.\n",
    "            affine_cache.append(a_cache_values)\n",
    "            batch_cache.append(b_cache_values)\n",
    "            relu_cache.append(r_cache_values)\n",
    "\n",
    "        # For the last layer, compute affine, append last layer cache, then compute the loss and dL\n",
    "        x, w, b = layer_out[-1], model_params['W'+str(num_layers)], model_params['b'+str(num_layers)]\n",
    "        a_values, a_cache_values = nn.affine_forward(x, w, b)\n",
    "        affine_cache.append(a_cache_values)\n",
    "        loss, dL = nn.softmax(a_values, y)\n",
    "        #loss += 0.05 * reg_strength * np.sum([np.sum(W**2) for key, W in model_params.items() if 'W' in key])\n",
    "        return loss, dL, affine_cache, batch_cache, relu_cache\n",
    "\n",
    "    def backward_pass(self, loss, dL, affine_cache, batch_cache, relu_cache):\n",
    "        # Now, lets implement the backward pass, with batchnorm in mind.\n",
    "        # This is a improved version of the above backward pass.\n",
    "        grads = {}\n",
    "        doutx = dL\n",
    "\n",
    "        # Loop backwards through the layers, computing the gradients and updating the grads dict.\n",
    "        for layer in reversed(range(num_layers)):\n",
    "            doutx, grads['W'+str(layer+1)], grads['b'+str(layer+1)] = nn.affine_backward(doutx, affine_cache[layer])\n",
    "            #grads['W'+str(layer+1)] *= (reg_strength * model_params['W'+str(layer+1)])\n",
    "\n",
    "        # As long as the layer is not the first one, compute the relu backward pass, then batchnorm backward pass.\n",
    "            if layer != 0:\n",
    "                doutx = nn.relu_backward(doutx, relu_cache[layer-1])\n",
    "                doutx, grads['gamma'+str(layer)], grads['beta'+str(layer)] = nn.batchnorm_backward(doutx, batch_cache[layer-1])\n",
    "        return grads\n",
    "    \n",
    "    def sgd(self, grads, model_params):\n",
    "        # This is simple stochastic gradient descent. Update the parameters (weights, biases, gamma and beta).\n",
    "        for layer in range(num_layers-1):\n",
    "                model_params['W'+str(layer+1)] -= learning_rate * grads['W'+str(layer+1)]\n",
    "                model_params['b'+str(layer+1)] -= learning_rate * grads['b'+str(layer+1)]\n",
    "                model_params['gamma'+str(layer+1)] -= learning_rate * grads['gamma'+str(layer+1)]\n",
    "                model_params['beta'+str(layer+1)] -= learning_rate * grads['beta'+str(layer+1)]\n",
    "        model_params['W'+str(layer+1)] -= learning_rate * grads['W'+str(layer+1)]\n",
    "        model_params['b'+str(layer+1)] -= learning_rate * grads['b'+str(layer+1)]\n",
    "        return model_params\n",
    "    \n",
    "    def gradient_descent(self, grads, model_params, momentum):\n",
    "        # This will be sgd + momentum. \n",
    "        for layer in range(num_layers):\n",
    "            W = model_params['W'+str(layer+1)]\n",
    "            grad_w = grads['W'+str(layer+1)]\n",
    "            v = np.zeros_like(W)\n",
    "\n",
    "            # Velocity update.\n",
    "            v = momentum * v - learning_rate * grad_w\n",
    "            W = W + v\n",
    "            model_params['W'+str(layer+1)] = W\n",
    "        return model_params"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimize, update, repeat. Experimentation with different hyperparameters (particularly the learning rate), as well as varying optimizers (sgd vs sgd+momentum, vs. rmsprop, etc.)\n",
    "\n",
    "Note: when using batch normalization, you can make the learning rate higher than if you werent using it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  0 lr:  1e-05 weight_scale:  0.01 loss:  2.3019301809191823\n",
      "iterations:  10 lr:  1e-05 weight_scale:  0.01 loss:  2.301169602725455\n",
      "iterations:  20 lr:  1e-05 weight_scale:  0.01 loss:  2.300410862775488\n",
      "iterations:  30 lr:  1e-05 weight_scale:  0.01 loss:  2.2996517869947035\n",
      "iterations:  40 lr:  1e-05 weight_scale:  0.01 loss:  2.298893541415861\n",
      "iterations:  50 lr:  1e-05 weight_scale:  0.01 loss:  2.2981420600565143\n",
      "iterations:  60 lr:  1e-05 weight_scale:  0.01 loss:  2.2973953317763733\n",
      "iterations:  70 lr:  1e-05 weight_scale:  0.01 loss:  2.2966513842469616\n",
      "iterations:  80 lr:  1e-05 weight_scale:  0.01 loss:  2.2959110134290155\n",
      "iterations:  90 lr:  1e-05 weight_scale:  0.01 loss:  2.2951721309796396\n",
      "iterations:  0 lr:  1.2618568830660211e-05 weight_scale:  0.010808080808080808 loss:  2.300125052131873\n",
      "iterations:  10 lr:  1.2618568830660211e-05 weight_scale:  0.010808080808080808 loss:  2.298964372258126\n",
      "iterations:  20 lr:  1.2618568830660211e-05 weight_scale:  0.010808080808080808 loss:  2.2978100097795147\n",
      "iterations:  30 lr:  1.2618568830660211e-05 weight_scale:  0.010808080808080808 loss:  2.296658832931095\n",
      "iterations:  40 lr:  1.2618568830660211e-05 weight_scale:  0.010808080808080808 loss:  2.2955032377965168\n",
      "iterations:  50 lr:  1.2618568830660211e-05 weight_scale:  0.010808080808080808 loss:  2.294352621870639\n",
      "iterations:  60 lr:  1.2618568830660211e-05 weight_scale:  0.010808080808080808 loss:  2.2932126382557803\n",
      "iterations:  70 lr:  1.2618568830660211e-05 weight_scale:  0.010808080808080808 loss:  2.292073293319006\n",
      "iterations:  80 lr:  1.2618568830660211e-05 weight_scale:  0.010808080808080808 loss:  2.2909315063464173\n",
      "iterations:  90 lr:  1.2618568830660211e-05 weight_scale:  0.010808080808080808 loss:  2.2897982767791616\n",
      "iterations:  0 lr:  1.5922827933410938e-05 weight_scale:  0.011616161616161616 loss:  2.3191345641522396\n",
      "iterations:  10 lr:  1.5922827933410938e-05 weight_scale:  0.011616161616161616 loss:  2.3175075716901232\n",
      "iterations:  20 lr:  1.5922827933410938e-05 weight_scale:  0.011616161616161616 loss:  2.31588033814706\n",
      "iterations:  30 lr:  1.5922827933410938e-05 weight_scale:  0.011616161616161616 loss:  2.3142624176716606\n",
      "iterations:  40 lr:  1.5922827933410938e-05 weight_scale:  0.011616161616161616 loss:  2.3126544761444414\n",
      "iterations:  50 lr:  1.5922827933410938e-05 weight_scale:  0.011616161616161616 loss:  2.311059397575679\n",
      "iterations:  60 lr:  1.5922827933410938e-05 weight_scale:  0.011616161616161616 loss:  2.3094762200916277\n",
      "iterations:  70 lr:  1.5922827933410938e-05 weight_scale:  0.011616161616161616 loss:  2.3079078205095622\n",
      "iterations:  80 lr:  1.5922827933410938e-05 weight_scale:  0.011616161616161616 loss:  2.306348087965257\n",
      "iterations:  90 lr:  1.5922827933410938e-05 weight_scale:  0.011616161616161616 loss:  2.304796787956097\n",
      "iterations:  0 lr:  2.0092330025650458e-05 weight_scale:  0.012424242424242424 loss:  2.2984368534866433\n",
      "iterations:  10 lr:  2.0092330025650458e-05 weight_scale:  0.012424242424242424 loss:  2.296823563433896\n",
      "iterations:  20 lr:  2.0092330025650458e-05 weight_scale:  0.012424242424242424 loss:  2.2952155359869435\n",
      "iterations:  30 lr:  2.0092330025650458e-05 weight_scale:  0.012424242424242424 loss:  2.2936267465814195\n",
      "iterations:  40 lr:  2.0092330025650458e-05 weight_scale:  0.012424242424242424 loss:  2.2920508557146326\n",
      "iterations:  50 lr:  2.0092330025650458e-05 weight_scale:  0.012424242424242424 loss:  2.290485415992986\n",
      "iterations:  60 lr:  2.0092330025650458e-05 weight_scale:  0.012424242424242424 loss:  2.288939742695711\n",
      "iterations:  70 lr:  2.0092330025650458e-05 weight_scale:  0.012424242424242424 loss:  2.2874105411918344\n",
      "iterations:  80 lr:  2.0092330025650458e-05 weight_scale:  0.012424242424242424 loss:  2.2858960866016047\n",
      "iterations:  90 lr:  2.0092330025650458e-05 weight_scale:  0.012424242424242424 loss:  2.284394582118872\n",
      "iterations:  0 lr:  2.5353644939701114e-05 weight_scale:  0.013232323232323233 loss:  2.31499634339461\n",
      "iterations:  10 lr:  2.5353644939701114e-05 weight_scale:  0.013232323232323233 loss:  2.3125724668901024\n",
      "iterations:  20 lr:  2.5353644939701114e-05 weight_scale:  0.013232323232323233 loss:  2.310177070640297\n",
      "iterations:  30 lr:  2.5353644939701114e-05 weight_scale:  0.013232323232323233 loss:  2.307792417859754\n",
      "iterations:  40 lr:  2.5353644939701114e-05 weight_scale:  0.013232323232323233 loss:  2.3054277089226782\n",
      "iterations:  50 lr:  2.5353644939701114e-05 weight_scale:  0.013232323232323233 loss:  2.303079240637407\n",
      "iterations:  60 lr:  2.5353644939701114e-05 weight_scale:  0.013232323232323233 loss:  2.3007367270342503\n",
      "iterations:  70 lr:  2.5353644939701114e-05 weight_scale:  0.013232323232323233 loss:  2.2984094265816006\n",
      "iterations:  80 lr:  2.5353644939701114e-05 weight_scale:  0.013232323232323233 loss:  2.296115197907416\n",
      "iterations:  90 lr:  2.5353644939701114e-05 weight_scale:  0.013232323232323233 loss:  2.293852215950716\n",
      "iterations:  0 lr:  3.199267137797385e-05 weight_scale:  0.01404040404040404 loss:  2.3062013885561115\n",
      "iterations:  10 lr:  3.199267137797385e-05 weight_scale:  0.01404040404040404 loss:  2.3032970229674716\n",
      "iterations:  20 lr:  3.199267137797385e-05 weight_scale:  0.01404040404040404 loss:  2.300404911219181\n",
      "iterations:  30 lr:  3.199267137797385e-05 weight_scale:  0.01404040404040404 loss:  2.297520766215548\n",
      "iterations:  40 lr:  3.199267137797385e-05 weight_scale:  0.01404040404040404 loss:  2.294651674428822\n",
      "iterations:  50 lr:  3.199267137797385e-05 weight_scale:  0.01404040404040404 loss:  2.291805515958772\n",
      "iterations:  60 lr:  3.199267137797385e-05 weight_scale:  0.01404040404040404 loss:  2.2889901533414525\n",
      "iterations:  70 lr:  3.199267137797385e-05 weight_scale:  0.01404040404040404 loss:  2.2862122924383264\n",
      "iterations:  80 lr:  3.199267137797385e-05 weight_scale:  0.01404040404040404 loss:  2.28348187975406\n",
      "iterations:  90 lr:  3.199267137797385e-05 weight_scale:  0.01404040404040404 loss:  2.280770264143735\n",
      "iterations:  0 lr:  4.037017258596558e-05 weight_scale:  0.014848484848484849 loss:  2.331314419020007\n",
      "iterations:  10 lr:  4.037017258596558e-05 weight_scale:  0.014848484848484849 loss:  2.328108717368158\n",
      "iterations:  20 lr:  4.037017258596558e-05 weight_scale:  0.014848484848484849 loss:  2.324917401699662\n",
      "iterations:  30 lr:  4.037017258596558e-05 weight_scale:  0.014848484848484849 loss:  2.3217320904126235\n",
      "iterations:  40 lr:  4.037017258596558e-05 weight_scale:  0.014848484848484849 loss:  2.318574104676883\n",
      "iterations:  50 lr:  4.037017258596558e-05 weight_scale:  0.014848484848484849 loss:  2.315450034571745\n",
      "iterations:  60 lr:  4.037017258596558e-05 weight_scale:  0.014848484848484849 loss:  2.312344190849468\n",
      "iterations:  70 lr:  4.037017258596558e-05 weight_scale:  0.014848484848484849 loss:  2.3092530124301938\n",
      "iterations:  80 lr:  4.037017258596558e-05 weight_scale:  0.014848484848484849 loss:  2.3061940875593776\n",
      "iterations:  90 lr:  4.037017258596558e-05 weight_scale:  0.014848484848484849 loss:  2.3031941878233413\n",
      "iterations:  0 lr:  5.0941380148163754e-05 weight_scale:  0.015656565656565657 loss:  2.3197725118559394\n",
      "iterations:  10 lr:  5.0941380148163754e-05 weight_scale:  0.015656565656565657 loss:  2.3152329706129122\n",
      "iterations:  20 lr:  5.0941380148163754e-05 weight_scale:  0.015656565656565657 loss:  2.3107551293173847\n",
      "iterations:  30 lr:  5.0941380148163754e-05 weight_scale:  0.015656565656565657 loss:  2.306310373821068\n",
      "iterations:  40 lr:  5.0941380148163754e-05 weight_scale:  0.015656565656565657 loss:  2.30195334698946\n",
      "iterations:  50 lr:  5.0941380148163754e-05 weight_scale:  0.015656565656565657 loss:  2.2976940900270257\n",
      "iterations:  60 lr:  5.0941380148163754e-05 weight_scale:  0.015656565656565657 loss:  2.293488976053198\n",
      "iterations:  70 lr:  5.0941380148163754e-05 weight_scale:  0.015656565656565657 loss:  2.289297526472302\n",
      "iterations:  80 lr:  5.0941380148163754e-05 weight_scale:  0.015656565656565657 loss:  2.285117120406451\n",
      "iterations:  90 lr:  5.0941380148163754e-05 weight_scale:  0.015656565656565657 loss:  2.280971642892172\n",
      "iterations:  0 lr:  6.428073117284319e-05 weight_scale:  0.016464646464646467 loss:  2.294716230744221\n",
      "iterations:  10 lr:  6.428073117284319e-05 weight_scale:  0.016464646464646467 loss:  2.2891135402725173\n",
      "iterations:  20 lr:  6.428073117284319e-05 weight_scale:  0.016464646464646467 loss:  2.2835736380307115\n",
      "iterations:  30 lr:  6.428073117284319e-05 weight_scale:  0.016464646464646467 loss:  2.278133464562521\n",
      "iterations:  40 lr:  6.428073117284319e-05 weight_scale:  0.016464646464646467 loss:  2.272800122915988\n",
      "iterations:  50 lr:  6.428073117284319e-05 weight_scale:  0.016464646464646467 loss:  2.267540087679737\n",
      "iterations:  60 lr:  6.428073117284319e-05 weight_scale:  0.016464646464646467 loss:  2.2623249186544303\n",
      "iterations:  70 lr:  6.428073117284319e-05 weight_scale:  0.016464646464646467 loss:  2.2571781951052956\n",
      "iterations:  80 lr:  6.428073117284319e-05 weight_scale:  0.016464646464646467 loss:  2.252060177194023\n",
      "iterations:  90 lr:  6.428073117284319e-05 weight_scale:  0.016464646464646467 loss:  2.247034798716045\n",
      "iterations:  0 lr:  8.111308307896872e-05 weight_scale:  0.017272727272727273 loss:  2.291555486655333\n",
      "iterations:  10 lr:  8.111308307896872e-05 weight_scale:  0.017272727272727273 loss:  2.2841777447277476\n",
      "iterations:  20 lr:  8.111308307896872e-05 weight_scale:  0.017272727272727273 loss:  2.2768925871726857\n",
      "iterations:  30 lr:  8.111308307896872e-05 weight_scale:  0.017272727272727273 loss:  2.2696589071319755\n",
      "iterations:  40 lr:  8.111308307896872e-05 weight_scale:  0.017272727272727273 loss:  2.2625210924001764\n",
      "iterations:  50 lr:  8.111308307896872e-05 weight_scale:  0.017272727272727273 loss:  2.255575183865874\n",
      "iterations:  60 lr:  8.111308307896872e-05 weight_scale:  0.017272727272727273 loss:  2.2487848726610125\n",
      "iterations:  70 lr:  8.111308307896872e-05 weight_scale:  0.017272727272727273 loss:  2.2421035897421775\n",
      "iterations:  80 lr:  8.111308307896872e-05 weight_scale:  0.017272727272727273 loss:  2.235533746598223\n",
      "iterations:  90 lr:  8.111308307896872e-05 weight_scale:  0.017272727272727273 loss:  2.229111680596782\n",
      "iterations:  0 lr:  0.00010235310218990269 weight_scale:  0.01808080808080808 loss:  2.296500170601771\n",
      "iterations:  10 lr:  0.00010235310218990269 weight_scale:  0.01808080808080808 loss:  2.2892047549681855\n",
      "iterations:  20 lr:  0.00010235310218990269 weight_scale:  0.01808080808080808 loss:  2.282016540346245\n",
      "iterations:  30 lr:  0.00010235310218990269 weight_scale:  0.01808080808080808 loss:  2.275036475965243\n",
      "iterations:  40 lr:  0.00010235310218990269 weight_scale:  0.01808080808080808 loss:  2.2681458652809408\n",
      "iterations:  50 lr:  0.00010235310218990269 weight_scale:  0.01808080808080808 loss:  2.261363801482896\n",
      "iterations:  60 lr:  0.00010235310218990269 weight_scale:  0.01808080808080808 loss:  2.2546967057223535\n",
      "iterations:  70 lr:  0.00010235310218990269 weight_scale:  0.01808080808080808 loss:  2.2481035081307006\n",
      "iterations:  80 lr:  0.00010235310218990269 weight_scale:  0.01808080808080808 loss:  2.241592735553491\n",
      "iterations:  90 lr:  0.00010235310218990269 weight_scale:  0.01808080808080808 loss:  2.235232475552369\n",
      "iterations:  0 lr:  0.0001291549665014884 weight_scale:  0.01888888888888889 loss:  2.327030709661052\n",
      "iterations:  10 lr:  0.0001291549665014884 weight_scale:  0.01888888888888889 loss:  2.317376571471888\n",
      "iterations:  20 lr:  0.0001291549665014884 weight_scale:  0.01888888888888889 loss:  2.308145266300493\n",
      "iterations:  30 lr:  0.0001291549665014884 weight_scale:  0.01888888888888889 loss:  2.2990827315515325\n",
      "iterations:  40 lr:  0.0001291549665014884 weight_scale:  0.01888888888888889 loss:  2.2901045475715724\n",
      "iterations:  50 lr:  0.0001291549665014884 weight_scale:  0.01888888888888889 loss:  2.281288259391857\n",
      "iterations:  60 lr:  0.0001291549665014884 weight_scale:  0.01888888888888889 loss:  2.2726609607880035\n",
      "iterations:  70 lr:  0.0001291549665014884 weight_scale:  0.01888888888888889 loss:  2.2642802830089717\n",
      "iterations:  80 lr:  0.0001291549665014884 weight_scale:  0.01888888888888889 loss:  2.256035938353206\n",
      "iterations:  90 lr:  0.0001291549665014884 weight_scale:  0.01888888888888889 loss:  2.248066611228509\n",
      "iterations:  0 lr:  0.00016297508346206434 weight_scale:  0.0196969696969697 loss:  2.3202353032969385\n",
      "iterations:  10 lr:  0.00016297508346206434 weight_scale:  0.0196969696969697 loss:  2.3063390177946186\n",
      "iterations:  20 lr:  0.00016297508346206434 weight_scale:  0.0196969696969697 loss:  2.292977797242891\n",
      "iterations:  30 lr:  0.00016297508346206434 weight_scale:  0.0196969696969697 loss:  2.279724599870196\n",
      "iterations:  40 lr:  0.00016297508346206434 weight_scale:  0.0196969696969697 loss:  2.2666016989305917\n",
      "iterations:  50 lr:  0.00016297508346206434 weight_scale:  0.0196969696969697 loss:  2.2538541074743788\n",
      "iterations:  60 lr:  0.00016297508346206434 weight_scale:  0.0196969696969697 loss:  2.2415513093418262\n",
      "iterations:  70 lr:  0.00016297508346206434 weight_scale:  0.0196969696969697 loss:  2.2297573052176123\n",
      "iterations:  80 lr:  0.00016297508346206434 weight_scale:  0.0196969696969697 loss:  2.218390869317424\n",
      "iterations:  90 lr:  0.00016297508346206434 weight_scale:  0.0196969696969697 loss:  2.2074450807004604\n",
      "iterations:  0 lr:  0.00020565123083486514 weight_scale:  0.020505050505050505 loss:  2.3515725666487794\n",
      "iterations:  10 lr:  0.00020565123083486514 weight_scale:  0.020505050505050505 loss:  2.3310278004030853\n",
      "iterations:  20 lr:  0.00020565123083486514 weight_scale:  0.020505050505050505 loss:  2.311330914769389\n",
      "iterations:  30 lr:  0.00020565123083486514 weight_scale:  0.020505050505050505 loss:  2.2923557135767467\n",
      "iterations:  40 lr:  0.00020565123083486514 weight_scale:  0.020505050505050505 loss:  2.2742050531342555\n",
      "iterations:  50 lr:  0.00020565123083486514 weight_scale:  0.020505050505050505 loss:  2.2569536520355173\n",
      "iterations:  60 lr:  0.00020565123083486514 weight_scale:  0.020505050505050505 loss:  2.2405134353239267\n",
      "iterations:  70 lr:  0.00020565123083486514 weight_scale:  0.020505050505050505 loss:  2.2245982879172423\n",
      "iterations:  80 lr:  0.00020565123083486514 weight_scale:  0.020505050505050505 loss:  2.2092848011859063\n",
      "iterations:  90 lr:  0.00020565123083486514 weight_scale:  0.020505050505050505 loss:  2.19442686785649\n",
      "iterations:  0 lr:  0.00025950242113997375 weight_scale:  0.02131313131313131 loss:  2.3138842268380118\n",
      "iterations:  10 lr:  0.00025950242113997375 weight_scale:  0.02131313131313131 loss:  2.292329716700797\n",
      "iterations:  20 lr:  0.00025950242113997375 weight_scale:  0.02131313131313131 loss:  2.2716781353247884\n",
      "iterations:  30 lr:  0.00025950242113997375 weight_scale:  0.02131313131313131 loss:  2.2515139361479393\n",
      "iterations:  40 lr:  0.00025950242113997375 weight_scale:  0.02131313131313131 loss:  2.232350035865407\n",
      "iterations:  50 lr:  0.00025950242113997375 weight_scale:  0.02131313131313131 loss:  2.2143108494511643\n",
      "iterations:  60 lr:  0.00025950242113997375 weight_scale:  0.02131313131313131 loss:  2.197002860880442\n",
      "iterations:  70 lr:  0.00025950242113997375 weight_scale:  0.02131313131313131 loss:  2.1804514607724226\n",
      "iterations:  80 lr:  0.00025950242113997375 weight_scale:  0.02131313131313131 loss:  2.1648589612062916\n",
      "iterations:  90 lr:  0.00025950242113997375 weight_scale:  0.02131313131313131 loss:  2.149959650464211\n",
      "iterations:  0 lr:  0.00032745491628777284 weight_scale:  0.02212121212121212 loss:  2.3417112510377427\n",
      "iterations:  10 lr:  0.00032745491628777284 weight_scale:  0.02212121212121212 loss:  2.3135975341790402\n",
      "iterations:  20 lr:  0.00032745491628777284 weight_scale:  0.02212121212121212 loss:  2.2864990843736077\n",
      "iterations:  30 lr:  0.00032745491628777284 weight_scale:  0.02212121212121212 loss:  2.260183380410093\n",
      "iterations:  40 lr:  0.00032745491628777284 weight_scale:  0.02212121212121212 loss:  2.234797916956178\n",
      "iterations:  50 lr:  0.00032745491628777284 weight_scale:  0.02212121212121212 loss:  2.2107551205377014\n",
      "iterations:  60 lr:  0.00032745491628777284 weight_scale:  0.02212121212121212 loss:  2.188433532776128\n",
      "iterations:  70 lr:  0.00032745491628777284 weight_scale:  0.02212121212121212 loss:  2.1674950749725945\n",
      "iterations:  80 lr:  0.00032745491628777284 weight_scale:  0.02212121212121212 loss:  2.147891474216477\n",
      "iterations:  90 lr:  0.00032745491628777284 weight_scale:  0.02212121212121212 loss:  2.129130591940748\n",
      "iterations:  0 lr:  0.00041320124001153346 weight_scale:  0.02292929292929293 loss:  2.305881341332489\n",
      "iterations:  10 lr:  0.00041320124001153346 weight_scale:  0.02292929292929293 loss:  2.2733562465003976\n",
      "iterations:  20 lr:  0.00041320124001153346 weight_scale:  0.02292929292929293 loss:  2.243031450116911\n",
      "iterations:  30 lr:  0.00041320124001153346 weight_scale:  0.02292929292929293 loss:  2.2148502588339283\n",
      "iterations:  40 lr:  0.00041320124001153346 weight_scale:  0.02292929292929293 loss:  2.189281651106455\n",
      "iterations:  50 lr:  0.00041320124001153346 weight_scale:  0.02292929292929293 loss:  2.165119613374165\n",
      "iterations:  60 lr:  0.00041320124001153346 weight_scale:  0.02292929292929293 loss:  2.141938333734727\n",
      "iterations:  70 lr:  0.00041320124001153346 weight_scale:  0.02292929292929293 loss:  2.1196166996007926\n",
      "iterations:  80 lr:  0.00041320124001153346 weight_scale:  0.02292929292929293 loss:  2.0983777111106265\n",
      "iterations:  90 lr:  0.00041320124001153346 weight_scale:  0.02292929292929293 loss:  2.0780560613157015\n",
      "iterations:  0 lr:  0.0005214008287999684 weight_scale:  0.023737373737373738 loss:  2.2939199036280566\n",
      "iterations:  10 lr:  0.0005214008287999684 weight_scale:  0.023737373737373738 loss:  2.2399425013306455\n",
      "iterations:  20 lr:  0.0005214008287999684 weight_scale:  0.023737373737373738 loss:  2.191625309686702\n",
      "iterations:  30 lr:  0.0005214008287999684 weight_scale:  0.023737373737373738 loss:  2.1478735254088392\n",
      "iterations:  40 lr:  0.0005214008287999684 weight_scale:  0.023737373737373738 loss:  2.108118937579997\n",
      "iterations:  50 lr:  0.0005214008287999684 weight_scale:  0.023737373737373738 loss:  2.072267806518407\n",
      "iterations:  60 lr:  0.0005214008287999684 weight_scale:  0.023737373737373738 loss:  2.0382475816131733\n",
      "iterations:  70 lr:  0.0005214008287999684 weight_scale:  0.023737373737373738 loss:  2.006806046720597\n",
      "iterations:  80 lr:  0.0005214008287999684 weight_scale:  0.023737373737373738 loss:  1.9779969272889133\n",
      "iterations:  90 lr:  0.0005214008287999684 weight_scale:  0.023737373737373738 loss:  1.9516507575587085\n",
      "iterations:  0 lr:  0.0006579332246575682 weight_scale:  0.024545454545454544 loss:  2.2991391769606335\n",
      "iterations:  10 lr:  0.0006579332246575682 weight_scale:  0.024545454545454544 loss:  2.2442318858104175\n",
      "iterations:  20 lr:  0.0006579332246575682 weight_scale:  0.024545454545454544 loss:  2.1937295264357775\n",
      "iterations:  30 lr:  0.0006579332246575682 weight_scale:  0.024545454545454544 loss:  2.148000384194453\n",
      "iterations:  40 lr:  0.0006579332246575682 weight_scale:  0.024545454545454544 loss:  2.1078972234119235\n",
      "iterations:  50 lr:  0.0006579332246575682 weight_scale:  0.024545454545454544 loss:  2.071517415795962\n",
      "iterations:  60 lr:  0.0006579332246575682 weight_scale:  0.024545454545454544 loss:  2.038645300599602\n",
      "iterations:  70 lr:  0.0006579332246575682 weight_scale:  0.024545454545454544 loss:  2.00923604726019\n",
      "iterations:  80 lr:  0.0006579332246575682 weight_scale:  0.024545454545454544 loss:  1.9825160145090372\n",
      "iterations:  90 lr:  0.0006579332246575682 weight_scale:  0.024545454545454544 loss:  1.9579368231261487\n",
      "iterations:  0 lr:  0.0008302175681319744 weight_scale:  0.025353535353535354 loss:  2.3135381208314367\n",
      "iterations:  10 lr:  0.0008302175681319744 weight_scale:  0.025353535353535354 loss:  2.246273897895253\n",
      "iterations:  20 lr:  0.0008302175681319744 weight_scale:  0.025353535353535354 loss:  2.186738573777469\n",
      "iterations:  30 lr:  0.0008302175681319744 weight_scale:  0.025353535353535354 loss:  2.1322340506288353\n",
      "iterations:  40 lr:  0.0008302175681319744 weight_scale:  0.025353535353535354 loss:  2.0823675513777142\n",
      "iterations:  50 lr:  0.0008302175681319744 weight_scale:  0.025353535353535354 loss:  2.037979546477667\n",
      "iterations:  60 lr:  0.0008302175681319744 weight_scale:  0.025353535353535354 loss:  1.9973582169266766\n",
      "iterations:  70 lr:  0.0008302175681319744 weight_scale:  0.025353535353535354 loss:  1.960482315792278\n",
      "iterations:  80 lr:  0.0008302175681319744 weight_scale:  0.025353535353535354 loss:  1.9263976239113683\n",
      "iterations:  90 lr:  0.0008302175681319744 weight_scale:  0.025353535353535354 loss:  1.8943035746372145\n",
      "iterations:  0 lr:  0.001047615752789665 weight_scale:  0.026161616161616164 loss:  2.329845346010557\n",
      "iterations:  10 lr:  0.001047615752789665 weight_scale:  0.026161616161616164 loss:  2.2489749386312115\n",
      "iterations:  20 lr:  0.001047615752789665 weight_scale:  0.026161616161616164 loss:  2.177570825463599\n",
      "iterations:  30 lr:  0.001047615752789665 weight_scale:  0.026161616161616164 loss:  2.115727238199905\n",
      "iterations:  40 lr:  0.001047615752789665 weight_scale:  0.026161616161616164 loss:  2.0617283290003625\n",
      "iterations:  50 lr:  0.001047615752789665 weight_scale:  0.026161616161616164 loss:  2.0139917437558092\n",
      "iterations:  60 lr:  0.001047615752789665 weight_scale:  0.026161616161616164 loss:  1.9721868783669843\n",
      "iterations:  70 lr:  0.001047615752789665 weight_scale:  0.026161616161616164 loss:  1.9334935155066248\n",
      "iterations:  80 lr:  0.001047615752789665 weight_scale:  0.026161616161616164 loss:  1.8983645989666913\n",
      "iterations:  90 lr:  0.001047615752789665 weight_scale:  0.026161616161616164 loss:  1.8665071550811434\n",
      "iterations:  0 lr:  0.0013219411484660286 weight_scale:  0.02696969696969697 loss:  2.3030440674726633\n",
      "iterations:  10 lr:  0.0013219411484660286 weight_scale:  0.02696969696969697 loss:  2.1969574475018065\n",
      "iterations:  20 lr:  0.0013219411484660286 weight_scale:  0.02696969696969697 loss:  2.1093888291556535\n",
      "iterations:  30 lr:  0.0013219411484660286 weight_scale:  0.02696969696969697 loss:  2.0382761353979584\n",
      "iterations:  40 lr:  0.0013219411484660286 weight_scale:  0.02696969696969697 loss:  1.9767785647013394\n",
      "iterations:  50 lr:  0.0013219411484660286 weight_scale:  0.02696969696969697 loss:  1.922713678091994\n",
      "iterations:  60 lr:  0.0013219411484660286 weight_scale:  0.02696969696969697 loss:  1.8753140630475311\n",
      "iterations:  70 lr:  0.0013219411484660286 weight_scale:  0.02696969696969697 loss:  1.835306863008874\n",
      "iterations:  80 lr:  0.0013219411484660286 weight_scale:  0.02696969696969697 loss:  1.7997787363227362\n",
      "iterations:  90 lr:  0.0013219411484660286 weight_scale:  0.02696969696969697 loss:  1.7667191985287434\n",
      "iterations:  0 lr:  0.0016681005372000592 weight_scale:  0.027777777777777776 loss:  2.300019034081886\n",
      "iterations:  10 lr:  0.0016681005372000592 weight_scale:  0.027777777777777776 loss:  2.1596020318897833\n",
      "iterations:  20 lr:  0.0016681005372000592 weight_scale:  0.027777777777777776 loss:  2.048600135827201\n",
      "iterations:  30 lr:  0.0016681005372000592 weight_scale:  0.027777777777777776 loss:  1.9576868507117828\n",
      "iterations:  40 lr:  0.0016681005372000592 weight_scale:  0.027777777777777776 loss:  1.8845266775474376\n",
      "iterations:  50 lr:  0.0016681005372000592 weight_scale:  0.027777777777777776 loss:  1.8234821585322256\n",
      "iterations:  60 lr:  0.0016681005372000592 weight_scale:  0.027777777777777776 loss:  1.772398228435533\n",
      "iterations:  70 lr:  0.0016681005372000592 weight_scale:  0.027777777777777776 loss:  1.7291375105340165\n",
      "iterations:  80 lr:  0.0016681005372000592 weight_scale:  0.027777777777777776 loss:  1.6909865566938906\n",
      "iterations:  90 lr:  0.0016681005372000592 weight_scale:  0.027777777777777776 loss:  1.6564268652778347\n",
      "iterations:  0 lr:  0.00210490414451202 weight_scale:  0.02858585858585859 loss:  2.292163965784531\n",
      "iterations:  10 lr:  0.00210490414451202 weight_scale:  0.02858585858585859 loss:  2.133204782354386\n",
      "iterations:  20 lr:  0.00210490414451202 weight_scale:  0.02858585858585859 loss:  2.0125190532235915\n",
      "iterations:  30 lr:  0.00210490414451202 weight_scale:  0.02858585858585859 loss:  1.9207143113547551\n",
      "iterations:  40 lr:  0.00210490414451202 weight_scale:  0.02858585858585859 loss:  1.8457577755710781\n",
      "iterations:  50 lr:  0.00210490414451202 weight_scale:  0.02858585858585859 loss:  1.781895410265185\n",
      "iterations:  60 lr:  0.00210490414451202 weight_scale:  0.02858585858585859 loss:  1.7270521753480927\n",
      "iterations:  70 lr:  0.00210490414451202 weight_scale:  0.02858585858585859 loss:  1.6793634572838363\n",
      "iterations:  80 lr:  0.00210490414451202 weight_scale:  0.02858585858585859 loss:  1.6384204991223672\n",
      "iterations:  90 lr:  0.00210490414451202 weight_scale:  0.02858585858585859 loss:  1.6030620780078946\n",
      "iterations:  0 lr:  0.0026560877829466868 weight_scale:  0.029393939393939396 loss:  2.33467785273192\n",
      "iterations:  10 lr:  0.0026560877829466868 weight_scale:  0.029393939393939396 loss:  2.1446248467326687\n",
      "iterations:  20 lr:  0.0026560877829466868 weight_scale:  0.029393939393939396 loss:  2.004596627427278\n",
      "iterations:  30 lr:  0.0026560877829466868 weight_scale:  0.029393939393939396 loss:  1.8978287118266157\n",
      "iterations:  40 lr:  0.0026560877829466868 weight_scale:  0.029393939393939396 loss:  1.8121378810046238\n",
      "iterations:  50 lr:  0.0026560877829466868 weight_scale:  0.029393939393939396 loss:  1.7414579090195905\n",
      "iterations:  60 lr:  0.0026560877829466868 weight_scale:  0.029393939393939396 loss:  1.68213590598611\n",
      "iterations:  70 lr:  0.0026560877829466868 weight_scale:  0.029393939393939396 loss:  1.6327411900771114\n",
      "iterations:  80 lr:  0.0026560877829466868 weight_scale:  0.029393939393939396 loss:  1.590924454977582\n",
      "iterations:  90 lr:  0.0026560877829466868 weight_scale:  0.029393939393939396 loss:  1.5543393474821445\n",
      "iterations:  0 lr:  0.003351602650938841 weight_scale:  0.030202020202020202 loss:  2.3282052538310865\n",
      "iterations:  10 lr:  0.003351602650938841 weight_scale:  0.030202020202020202 loss:  2.088824331470042\n",
      "iterations:  20 lr:  0.003351602650938841 weight_scale:  0.030202020202020202 loss:  1.9273184383579147\n",
      "iterations:  30 lr:  0.003351602650938841 weight_scale:  0.030202020202020202 loss:  1.8118012261456817\n",
      "iterations:  40 lr:  0.003351602650938841 weight_scale:  0.030202020202020202 loss:  1.7230665594058932\n",
      "iterations:  50 lr:  0.003351602650938841 weight_scale:  0.030202020202020202 loss:  1.6535726115960452\n",
      "iterations:  60 lr:  0.003351602650938841 weight_scale:  0.030202020202020202 loss:  1.5976052552293944\n",
      "iterations:  70 lr:  0.003351602650938841 weight_scale:  0.030202020202020202 loss:  1.5520368735886345\n",
      "iterations:  80 lr:  0.003351602650938841 weight_scale:  0.030202020202020202 loss:  1.5134692109790386\n",
      "iterations:  90 lr:  0.003351602650938841 weight_scale:  0.030202020202020202 loss:  1.4792521014435354\n",
      "iterations:  0 lr:  0.0042292428743894986 weight_scale:  0.03101010101010101 loss:  2.288109793887122\n",
      "iterations:  10 lr:  0.0042292428743894986 weight_scale:  0.03101010101010101 loss:  2.0071542815761365\n",
      "iterations:  20 lr:  0.0042292428743894986 weight_scale:  0.03101010101010101 loss:  1.8415806123749627\n",
      "iterations:  30 lr:  0.0042292428743894986 weight_scale:  0.03101010101010101 loss:  1.7249047139995761\n",
      "iterations:  40 lr:  0.0042292428743894986 weight_scale:  0.03101010101010101 loss:  1.6377067095233468\n",
      "iterations:  50 lr:  0.0042292428743894986 weight_scale:  0.03101010101010101 loss:  1.5702703686424246\n",
      "iterations:  60 lr:  0.0042292428743894986 weight_scale:  0.03101010101010101 loss:  1.5152849291621637\n",
      "iterations:  70 lr:  0.0042292428743894986 weight_scale:  0.03101010101010101 loss:  1.4701549060001986\n",
      "iterations:  80 lr:  0.0042292428743894986 weight_scale:  0.03101010101010101 loss:  1.4319077039728856\n",
      "iterations:  90 lr:  0.0042292428743894986 weight_scale:  0.03101010101010101 loss:  1.3992176174469941\n",
      "iterations:  0 lr:  0.005336699231206307 weight_scale:  0.03181818181818182 loss:  2.32533978038766\n",
      "iterations:  10 lr:  0.005336699231206307 weight_scale:  0.03181818181818182 loss:  1.96773722783253\n",
      "iterations:  20 lr:  0.005336699231206307 weight_scale:  0.03181818181818182 loss:  1.7649537035257996\n",
      "iterations:  30 lr:  0.005336699231206307 weight_scale:  0.03181818181818182 loss:  1.629217736502486\n",
      "iterations:  40 lr:  0.005336699231206307 weight_scale:  0.03181818181818182 loss:  1.5342441032397807\n",
      "iterations:  50 lr:  0.005336699231206307 weight_scale:  0.03181818181818182 loss:  1.4655661952638135\n",
      "iterations:  60 lr:  0.005336699231206307 weight_scale:  0.03181818181818182 loss:  1.4129003209674895\n",
      "iterations:  70 lr:  0.005336699231206307 weight_scale:  0.03181818181818182 loss:  1.3702292517690577\n",
      "iterations:  80 lr:  0.005336699231206307 weight_scale:  0.03181818181818182 loss:  1.3345542243435313\n",
      "iterations:  90 lr:  0.005336699231206307 weight_scale:  0.03181818181818182 loss:  1.3039430112588104\n",
      "iterations:  0 lr:  0.006734150657750821 weight_scale:  0.03262626262626263 loss:  2.300953796603964\n",
      "iterations:  10 lr:  0.006734150657750821 weight_scale:  0.03262626262626263 loss:  1.8773734822504533\n",
      "iterations:  20 lr:  0.006734150657750821 weight_scale:  0.03262626262626263 loss:  1.6772494100977762\n",
      "iterations:  30 lr:  0.006734150657750821 weight_scale:  0.03262626262626263 loss:  1.55556258701083\n",
      "iterations:  40 lr:  0.006734150657750821 weight_scale:  0.03262626262626263 loss:  1.468244360192217\n",
      "iterations:  50 lr:  0.006734150657750821 weight_scale:  0.03262626262626263 loss:  1.4020167570564797\n",
      "iterations:  60 lr:  0.006734150657750821 weight_scale:  0.03262626262626263 loss:  1.3506879337888862\n",
      "iterations:  70 lr:  0.006734150657750821 weight_scale:  0.03262626262626263 loss:  1.3089654239137054\n",
      "iterations:  80 lr:  0.006734150657750821 weight_scale:  0.03262626262626263 loss:  1.2757702326811566\n",
      "iterations:  90 lr:  0.006734150657750821 weight_scale:  0.03262626262626263 loss:  1.2484447912271255\n",
      "iterations:  0 lr:  0.008497534359086439 weight_scale:  0.033434343434343435 loss:  2.3233707473716247\n",
      "iterations:  10 lr:  0.008497534359086439 weight_scale:  0.033434343434343435 loss:  1.8447448516164198\n",
      "iterations:  20 lr:  0.008497534359086439 weight_scale:  0.033434343434343435 loss:  1.602887180490611\n",
      "iterations:  30 lr:  0.008497534359086439 weight_scale:  0.033434343434343435 loss:  1.4589184500889507\n",
      "iterations:  40 lr:  0.008497534359086439 weight_scale:  0.033434343434343435 loss:  1.3601954222436594\n",
      "iterations:  50 lr:  0.008497534359086439 weight_scale:  0.033434343434343435 loss:  1.2902110908774689\n",
      "iterations:  60 lr:  0.008497534359086439 weight_scale:  0.033434343434343435 loss:  1.239712360400107\n",
      "iterations:  70 lr:  0.008497534359086439 weight_scale:  0.033434343434343435 loss:  1.200693280649455\n",
      "iterations:  80 lr:  0.008497534359086439 weight_scale:  0.033434343434343435 loss:  1.1690887823619744\n",
      "iterations:  90 lr:  0.008497534359086439 weight_scale:  0.033434343434343435 loss:  1.1426941795357568\n",
      "iterations:  0 lr:  0.010722672220103232 weight_scale:  0.03424242424242424 loss:  2.3443641979760876\n",
      "iterations:  10 lr:  0.010722672220103232 weight_scale:  0.03424242424242424 loss:  1.7839306992616684\n",
      "iterations:  20 lr:  0.010722672220103232 weight_scale:  0.03424242424242424 loss:  1.5383412308539175\n",
      "iterations:  30 lr:  0.010722672220103232 weight_scale:  0.03424242424242424 loss:  1.3931593645672151\n",
      "iterations:  40 lr:  0.010722672220103232 weight_scale:  0.03424242424242424 loss:  1.3036043076016406\n",
      "iterations:  50 lr:  0.010722672220103232 weight_scale:  0.03424242424242424 loss:  1.2419186196806995\n",
      "iterations:  60 lr:  0.010722672220103232 weight_scale:  0.03424242424242424 loss:  1.1978858957346117\n",
      "iterations:  70 lr:  0.010722672220103232 weight_scale:  0.03424242424242424 loss:  1.1657968223146569\n",
      "iterations:  80 lr:  0.010722672220103232 weight_scale:  0.03424242424242424 loss:  1.1411854754970232\n",
      "iterations:  90 lr:  0.010722672220103232 weight_scale:  0.03424242424242424 loss:  1.121043093767097\n",
      "iterations:  0 lr:  0.013530477745798061 weight_scale:  0.035050505050505054 loss:  2.349442875752448\n",
      "iterations:  10 lr:  0.013530477745798061 weight_scale:  0.035050505050505054 loss:  1.7094696882701803\n",
      "iterations:  20 lr:  0.013530477745798061 weight_scale:  0.035050505050505054 loss:  1.4645413685467217\n",
      "iterations:  30 lr:  0.013530477745798061 weight_scale:  0.035050505050505054 loss:  1.3336262392460503\n",
      "iterations:  40 lr:  0.013530477745798061 weight_scale:  0.035050505050505054 loss:  1.2466626709377144\n",
      "iterations:  50 lr:  0.013530477745798061 weight_scale:  0.035050505050505054 loss:  1.1819353463629967\n",
      "iterations:  60 lr:  0.013530477745798061 weight_scale:  0.035050505050505054 loss:  1.134474587627163\n",
      "iterations:  70 lr:  0.013530477745798061 weight_scale:  0.035050505050505054 loss:  1.1011187523429098\n",
      "iterations:  80 lr:  0.013530477745798061 weight_scale:  0.035050505050505054 loss:  1.0742934515353268\n",
      "iterations:  90 lr:  0.013530477745798061 weight_scale:  0.035050505050505054 loss:  1.055170886421874\n",
      "iterations:  0 lr:  0.017073526474706904 weight_scale:  0.03585858585858586 loss:  2.337579583780712\n",
      "iterations:  10 lr:  0.017073526474706904 weight_scale:  0.03585858585858586 loss:  1.611644867713433\n",
      "iterations:  20 lr:  0.017073526474706904 weight_scale:  0.03585858585858586 loss:  1.3774180734296297\n",
      "iterations:  30 lr:  0.017073526474706904 weight_scale:  0.03585858585858586 loss:  1.2515829281227884\n",
      "iterations:  40 lr:  0.017073526474706904 weight_scale:  0.03585858585858586 loss:  1.1738991976494113\n",
      "iterations:  50 lr:  0.017073526474706904 weight_scale:  0.03585858585858586 loss:  1.1198096701028228\n",
      "iterations:  60 lr:  0.017073526474706904 weight_scale:  0.03585858585858586 loss:  1.080226080905468\n",
      "iterations:  70 lr:  0.017073526474706904 weight_scale:  0.03585858585858586 loss:  1.051354529948613\n",
      "iterations:  80 lr:  0.017073526474706904 weight_scale:  0.03585858585858586 loss:  1.0287522606437027\n",
      "iterations:  90 lr:  0.017073526474706904 weight_scale:  0.03585858585858586 loss:  1.0096559368674223\n",
      "iterations:  0 lr:  0.021544346900318846 weight_scale:  0.03666666666666667 loss:  2.343804145299245\n",
      "iterations:  10 lr:  0.021544346900318846 weight_scale:  0.03666666666666667 loss:  1.5338292926160668\n",
      "iterations:  20 lr:  0.021544346900318846 weight_scale:  0.03666666666666667 loss:  1.309363812234003\n",
      "iterations:  30 lr:  0.021544346900318846 weight_scale:  0.03666666666666667 loss:  1.1928115055575044\n",
      "iterations:  40 lr:  0.021544346900318846 weight_scale:  0.03666666666666667 loss:  1.1232694978022477\n",
      "iterations:  50 lr:  0.021544346900318846 weight_scale:  0.03666666666666667 loss:  1.0745108585950032\n",
      "iterations:  60 lr:  0.021544346900318846 weight_scale:  0.03666666666666667 loss:  1.039909913528119\n",
      "iterations:  70 lr:  0.021544346900318846 weight_scale:  0.03666666666666667 loss:  1.0123960241878989\n",
      "iterations:  80 lr:  0.021544346900318846 weight_scale:  0.03666666666666667 loss:  0.9919107065386599\n",
      "iterations:  90 lr:  0.021544346900318846 weight_scale:  0.03666666666666667 loss:  0.975503046794803\n",
      "iterations:  0 lr:  0.027185882427329403 weight_scale:  0.03747474747474747 loss:  2.277906748273564\n",
      "iterations:  10 lr:  0.027185882427329403 weight_scale:  0.03747474747474747 loss:  1.397546720037414\n",
      "iterations:  20 lr:  0.027185882427329403 weight_scale:  0.03747474747474747 loss:  1.1840272572595167\n",
      "iterations:  30 lr:  0.027185882427329403 weight_scale:  0.03747474747474747 loss:  1.0785618813410909\n",
      "iterations:  40 lr:  0.027185882427329403 weight_scale:  0.03747474747474747 loss:  1.0136679241501911\n",
      "iterations:  50 lr:  0.027185882427329403 weight_scale:  0.03747474747474747 loss:  0.9698830820175312\n",
      "iterations:  60 lr:  0.027185882427329403 weight_scale:  0.03747474747474747 loss:  0.938264404379175\n",
      "iterations:  70 lr:  0.027185882427329403 weight_scale:  0.03747474747474747 loss:  0.9163151237336797\n",
      "iterations:  80 lr:  0.027185882427329403 weight_scale:  0.03747474747474747 loss:  0.8992613501025862\n",
      "iterations:  90 lr:  0.027185882427329403 weight_scale:  0.03747474747474747 loss:  0.8858368600221234\n",
      "iterations:  0 lr:  0.03430469286314919 weight_scale:  0.038282828282828286 loss:  2.322359496992897\n",
      "iterations:  10 lr:  0.03430469286314919 weight_scale:  0.038282828282828286 loss:  1.3151820792482514\n",
      "iterations:  20 lr:  0.03430469286314919 weight_scale:  0.038282828282828286 loss:  1.0965627422344153\n",
      "iterations:  30 lr:  0.03430469286314919 weight_scale:  0.038282828282828286 loss:  0.9953540320231282\n",
      "iterations:  40 lr:  0.03430469286314919 weight_scale:  0.038282828282828286 loss:  0.9396974661831511\n",
      "iterations:  50 lr:  0.03430469286314919 weight_scale:  0.038282828282828286 loss:  0.9034565606019587\n",
      "iterations:  60 lr:  0.03430469286314919 weight_scale:  0.038282828282828286 loss:  0.8763817279235218\n",
      "iterations:  70 lr:  0.03430469286314919 weight_scale:  0.038282828282828286 loss:  0.8588078156368598\n",
      "iterations:  80 lr:  0.03430469286314919 weight_scale:  0.038282828282828286 loss:  0.8466790572460937\n",
      "iterations:  90 lr:  0.03430469286314919 weight_scale:  0.038282828282828286 loss:  0.8372038465205953\n",
      "iterations:  0 lr:  0.04328761281083057 weight_scale:  0.03909090909090909 loss:  2.383536239691488\n",
      "iterations:  10 lr:  0.04328761281083057 weight_scale:  0.03909090909090909 loss:  1.3963868111974644\n",
      "iterations:  20 lr:  0.04328761281083057 weight_scale:  0.03909090909090909 loss:  1.1689484683777034\n",
      "iterations:  30 lr:  0.04328761281083057 weight_scale:  0.03909090909090909 loss:  1.0642259996456793\n",
      "iterations:  40 lr:  0.04328761281083057 weight_scale:  0.03909090909090909 loss:  1.0059884122332408\n",
      "iterations:  50 lr:  0.04328761281083057 weight_scale:  0.03909090909090909 loss:  0.9709988291144426\n",
      "iterations:  60 lr:  0.04328761281083057 weight_scale:  0.03909090909090909 loss:  0.9488883798817744\n",
      "iterations:  70 lr:  0.04328761281083057 weight_scale:  0.03909090909090909 loss:  0.9324157348508162\n",
      "iterations:  80 lr:  0.04328761281083057 weight_scale:  0.03909090909090909 loss:  0.9196675676933332\n",
      "iterations:  90 lr:  0.04328761281083057 weight_scale:  0.03909090909090909 loss:  0.9105047528449616\n",
      "iterations:  0 lr:  0.05462277217684343 weight_scale:  0.0398989898989899 loss:  2.3606349997199767\n",
      "iterations:  10 lr:  0.05462277217684343 weight_scale:  0.0398989898989899 loss:  1.1551443122304854\n",
      "iterations:  20 lr:  0.05462277217684343 weight_scale:  0.0398989898989899 loss:  0.9552455656943415\n",
      "iterations:  30 lr:  0.05462277217684343 weight_scale:  0.0398989898989899 loss:  0.8648752357081474\n",
      "iterations:  40 lr:  0.05462277217684343 weight_scale:  0.0398989898989899 loss:  0.8181194884819923\n",
      "iterations:  50 lr:  0.05462277217684343 weight_scale:  0.0398989898989899 loss:  0.7894199332020936\n",
      "iterations:  60 lr:  0.05462277217684343 weight_scale:  0.0398989898989899 loss:  0.7736066756270225\n",
      "iterations:  70 lr:  0.05462277217684343 weight_scale:  0.0398989898989899 loss:  0.7624691574501256\n",
      "iterations:  80 lr:  0.05462277217684343 weight_scale:  0.0398989898989899 loss:  0.7544255179132702\n",
      "iterations:  90 lr:  0.05462277217684343 weight_scale:  0.0398989898989899 loss:  0.7482445178144215\n",
      "iterations:  0 lr:  0.06892612104349695 weight_scale:  0.040707070707070706 loss:  2.305647388834302\n",
      "iterations:  10 lr:  0.06892612104349695 weight_scale:  0.040707070707070706 loss:  1.1023104671056803\n",
      "iterations:  20 lr:  0.06892612104349695 weight_scale:  0.040707070707070706 loss:  0.9089960858269083\n",
      "iterations:  30 lr:  0.06892612104349695 weight_scale:  0.040707070707070706 loss:  0.8268277202005361\n",
      "iterations:  40 lr:  0.06892612104349695 weight_scale:  0.040707070707070706 loss:  0.7888725824616383\n",
      "iterations:  50 lr:  0.06892612104349695 weight_scale:  0.040707070707070706 loss:  0.7699568806124129\n",
      "iterations:  60 lr:  0.06892612104349695 weight_scale:  0.040707070707070706 loss:  0.7572618009333653\n",
      "iterations:  70 lr:  0.06892612104349695 weight_scale:  0.040707070707070706 loss:  0.7481750828086462\n",
      "iterations:  80 lr:  0.06892612104349695 weight_scale:  0.040707070707070706 loss:  0.7412346966903081\n",
      "iterations:  90 lr:  0.06892612104349695 weight_scale:  0.040707070707070706 loss:  0.7355852794563679\n",
      "iterations:  0 lr:  0.08697490026177834 weight_scale:  0.04151515151515152 loss:  2.2923540612554074\n",
      "iterations:  10 lr:  0.08697490026177834 weight_scale:  0.04151515151515152 loss:  0.9856646341671241\n",
      "iterations:  20 lr:  0.08697490026177834 weight_scale:  0.04151515151515152 loss:  0.828478120670246\n",
      "iterations:  30 lr:  0.08697490026177834 weight_scale:  0.04151515151515152 loss:  0.7697655207721766\n",
      "iterations:  40 lr:  0.08697490026177834 weight_scale:  0.04151515151515152 loss:  0.7394000449624218\n",
      "iterations:  50 lr:  0.08697490026177834 weight_scale:  0.04151515151515152 loss:  0.7229609766462982\n",
      "iterations:  60 lr:  0.08697490026177834 weight_scale:  0.04151515151515152 loss:  0.7124473251184142\n",
      "iterations:  70 lr:  0.08697490026177834 weight_scale:  0.04151515151515152 loss:  0.7046945590374272\n",
      "iterations:  80 lr:  0.08697490026177834 weight_scale:  0.04151515151515152 loss:  0.6983180379723711\n",
      "iterations:  90 lr:  0.08697490026177834 weight_scale:  0.04151515151515152 loss:  0.6928803216459954\n",
      "iterations:  0 lr:  0.10974987654930568 weight_scale:  0.042323232323232325 loss:  2.3934387838055597\n",
      "iterations:  10 lr:  0.10974987654930568 weight_scale:  0.042323232323232325 loss:  1.0301780958935882\n",
      "iterations:  20 lr:  0.10974987654930568 weight_scale:  0.042323232323232325 loss:  0.8833113246930495\n",
      "iterations:  30 lr:  0.10974987654930568 weight_scale:  0.042323232323232325 loss:  0.8300861278432734\n",
      "iterations:  40 lr:  0.10974987654930568 weight_scale:  0.042323232323232325 loss:  0.8056297262620119\n",
      "iterations:  50 lr:  0.10974987654930568 weight_scale:  0.042323232323232325 loss:  0.7899096449850667\n",
      "iterations:  60 lr:  0.10974987654930568 weight_scale:  0.042323232323232325 loss:  0.7791562463522038\n",
      "iterations:  70 lr:  0.10974987654930568 weight_scale:  0.042323232323232325 loss:  0.7712684872162672\n",
      "iterations:  80 lr:  0.10974987654930568 weight_scale:  0.042323232323232325 loss:  0.7648237903521555\n",
      "iterations:  90 lr:  0.10974987654930568 weight_scale:  0.042323232323232325 loss:  0.7592407379185143\n",
      "iterations:  0 lr:  0.13848863713938717 weight_scale:  0.04313131313131313 loss:  2.400380041023846\n",
      "iterations:  10 lr:  0.13848863713938717 weight_scale:  0.04313131313131313 loss:  0.9041940871211661\n",
      "iterations:  20 lr:  0.13848863713938717 weight_scale:  0.04313131313131313 loss:  0.7845416867047682\n",
      "iterations:  30 lr:  0.13848863713938717 weight_scale:  0.04313131313131313 loss:  0.741981736906448\n",
      "iterations:  40 lr:  0.13848863713938717 weight_scale:  0.04313131313131313 loss:  0.7222959127645459\n",
      "iterations:  50 lr:  0.13848863713938717 weight_scale:  0.04313131313131313 loss:  0.7098871743290739\n",
      "iterations:  60 lr:  0.13848863713938717 weight_scale:  0.04313131313131313 loss:  0.7006677481850713\n",
      "iterations:  70 lr:  0.13848863713938717 weight_scale:  0.04313131313131313 loss:  0.6930925275472862\n",
      "iterations:  80 lr:  0.13848863713938717 weight_scale:  0.04313131313131313 loss:  0.6864646462606723\n",
      "iterations:  90 lr:  0.13848863713938717 weight_scale:  0.04313131313131313 loss:  0.6804003123625584\n",
      "iterations:  0 lr:  0.1747528400007683 weight_scale:  0.04393939393939394 loss:  2.3373246504168117\n",
      "iterations:  10 lr:  0.1747528400007683 weight_scale:  0.04393939393939394 loss:  0.7717315357571337\n",
      "iterations:  20 lr:  0.1747528400007683 weight_scale:  0.04393939393939394 loss:  0.6617948443311635\n",
      "iterations:  30 lr:  0.1747528400007683 weight_scale:  0.04393939393939394 loss:  0.6243581950210082\n",
      "iterations:  40 lr:  0.1747528400007683 weight_scale:  0.04393939393939394 loss:  0.6069489307401104\n",
      "iterations:  50 lr:  0.1747528400007683 weight_scale:  0.04393939393939394 loss:  0.5950331387870768\n",
      "iterations:  60 lr:  0.1747528400007683 weight_scale:  0.04393939393939394 loss:  0.5856860725847812\n",
      "iterations:  70 lr:  0.1747528400007683 weight_scale:  0.04393939393939394 loss:  0.5777455819637125\n",
      "iterations:  80 lr:  0.1747528400007683 weight_scale:  0.04393939393939394 loss:  0.5706460864392334\n",
      "iterations:  90 lr:  0.1747528400007683 weight_scale:  0.04393939393939394 loss:  0.5641154377252812\n",
      "iterations:  0 lr:  0.22051307399030456 weight_scale:  0.04474747474747475 loss:  2.346997890810824\n",
      "iterations:  10 lr:  0.22051307399030456 weight_scale:  0.04474747474747475 loss:  0.7904209980864679\n",
      "iterations:  20 lr:  0.22051307399030456 weight_scale:  0.04474747474747475 loss:  0.6956186522919056\n",
      "iterations:  30 lr:  0.22051307399030456 weight_scale:  0.04474747474747475 loss:  0.6664021585437758\n",
      "iterations:  40 lr:  0.22051307399030456 weight_scale:  0.04474747474747475 loss:  0.649723985117985\n",
      "iterations:  50 lr:  0.22051307399030456 weight_scale:  0.04474747474747475 loss:  0.6375190127446824\n",
      "iterations:  60 lr:  0.22051307399030456 weight_scale:  0.04474747474747475 loss:  0.6273123919913902\n",
      "iterations:  70 lr:  0.22051307399030456 weight_scale:  0.04474747474747475 loss:  0.6182440405808601\n",
      "iterations:  80 lr:  0.22051307399030456 weight_scale:  0.04474747474747475 loss:  0.609875581988906\n",
      "iterations:  90 lr:  0.22051307399030456 weight_scale:  0.04474747474747475 loss:  0.6019866265116953\n",
      "iterations:  0 lr:  0.2782559402207126 weight_scale:  0.04555555555555556 loss:  2.3057446167376954\n",
      "iterations:  10 lr:  0.2782559402207126 weight_scale:  0.04555555555555556 loss:  0.7074604936964504\n",
      "iterations:  20 lr:  0.2782559402207126 weight_scale:  0.04555555555555556 loss:  0.6202623362105016\n",
      "iterations:  30 lr:  0.2782559402207126 weight_scale:  0.04555555555555556 loss:  0.5917704803645788\n",
      "iterations:  40 lr:  0.2782559402207126 weight_scale:  0.04555555555555556 loss:  0.5751558187222967\n",
      "iterations:  50 lr:  0.2782559402207126 weight_scale:  0.04555555555555556 loss:  0.5621911862176552\n",
      "iterations:  60 lr:  0.2782559402207126 weight_scale:  0.04555555555555556 loss:  0.5509538480430511\n",
      "iterations:  70 lr:  0.2782559402207126 weight_scale:  0.04555555555555556 loss:  0.5407059909113003\n",
      "iterations:  80 lr:  0.2782559402207126 weight_scale:  0.04555555555555556 loss:  0.5311220449023559\n",
      "iterations:  90 lr:  0.2782559402207126 weight_scale:  0.04555555555555556 loss:  0.5220275291027212\n",
      "iterations:  0 lr:  0.3511191734215127 weight_scale:  0.046363636363636364 loss:  2.37752471868242\n",
      "iterations:  10 lr:  0.3511191734215127 weight_scale:  0.046363636363636364 loss:  0.6730344234139054\n",
      "iterations:  20 lr:  0.3511191734215127 weight_scale:  0.046363636363636364 loss:  0.6066505100274155\n",
      "iterations:  30 lr:  0.3511191734215127 weight_scale:  0.046363636363636364 loss:  0.5800498692063697\n",
      "iterations:  40 lr:  0.3511191734215127 weight_scale:  0.046363636363636364 loss:  0.5618689416065605\n",
      "iterations:  50 lr:  0.3511191734215127 weight_scale:  0.046363636363636364 loss:  0.5467926295277314\n",
      "iterations:  60 lr:  0.3511191734215127 weight_scale:  0.046363636363636364 loss:  0.5335603372473464\n",
      "iterations:  70 lr:  0.3511191734215127 weight_scale:  0.046363636363636364 loss:  0.5213728484587526\n",
      "iterations:  80 lr:  0.3511191734215127 weight_scale:  0.046363636363636364 loss:  0.5099444268733793\n",
      "iterations:  90 lr:  0.3511191734215127 weight_scale:  0.046363636363636364 loss:  0.4990980047182748\n",
      "iterations:  0 lr:  0.44306214575838776 weight_scale:  0.04717171717171718 loss:  2.329671073597165\n",
      "iterations:  10 lr:  0.44306214575838776 weight_scale:  0.04717171717171718 loss:  0.653895894586934\n",
      "iterations:  20 lr:  0.44306214575838776 weight_scale:  0.04717171717171718 loss:  0.5823347123231523\n",
      "iterations:  30 lr:  0.44306214575838776 weight_scale:  0.04717171717171718 loss:  0.5557135237350213\n",
      "iterations:  40 lr:  0.44306214575838776 weight_scale:  0.04717171717171718 loss:  0.5366635058963998\n",
      "iterations:  50 lr:  0.44306214575838776 weight_scale:  0.04717171717171718 loss:  0.5205253041838024\n",
      "iterations:  60 lr:  0.44306214575838776 weight_scale:  0.04717171717171718 loss:  0.5059541606223799\n",
      "iterations:  70 lr:  0.44306214575838776 weight_scale:  0.04717171717171718 loss:  0.49243837426563497\n",
      "iterations:  80 lr:  0.44306214575838776 weight_scale:  0.04717171717171718 loss:  0.479738668841987\n",
      "iterations:  90 lr:  0.44306214575838776 weight_scale:  0.04717171717171718 loss:  0.467703725716403\n",
      "iterations:  0 lr:  0.5590810182512223 weight_scale:  0.04797979797979798 loss:  2.367538230544348\n",
      "iterations:  10 lr:  0.5590810182512223 weight_scale:  0.04797979797979798 loss:  0.5459134589733388\n",
      "iterations:  20 lr:  0.5590810182512223 weight_scale:  0.04797979797979798 loss:  0.4797191479940853\n",
      "iterations:  30 lr:  0.5590810182512223 weight_scale:  0.04797979797979798 loss:  0.4527599552359376\n",
      "iterations:  40 lr:  0.5590810182512223 weight_scale:  0.04797979797979798 loss:  0.4328982229267199\n",
      "iterations:  50 lr:  0.5590810182512223 weight_scale:  0.04797979797979798 loss:  0.4163755262236233\n",
      "iterations:  60 lr:  0.5590810182512223 weight_scale:  0.04797979797979798 loss:  0.40151671818014556\n",
      "iterations:  70 lr:  0.5590810182512223 weight_scale:  0.04797979797979798 loss:  0.38788144939055696\n",
      "iterations:  80 lr:  0.5590810182512223 weight_scale:  0.04797979797979798 loss:  0.3751904046265054\n",
      "iterations:  90 lr:  0.5590810182512223 weight_scale:  0.04797979797979798 loss:  0.3615563597500907\n",
      "iterations:  0 lr:  0.7054802310718645 weight_scale:  0.04878787878787879 loss:  2.3508625787846635\n",
      "iterations:  10 lr:  0.7054802310718645 weight_scale:  0.04878787878787879 loss:  0.532241936170127\n",
      "iterations:  20 lr:  0.7054802310718645 weight_scale:  0.04878787878787879 loss:  0.46834791068664466\n",
      "iterations:  30 lr:  0.7054802310718645 weight_scale:  0.04878787878787879 loss:  0.437844092683819\n",
      "iterations:  40 lr:  0.7054802310718645 weight_scale:  0.04878787878787879 loss:  0.414637275941868\n",
      "iterations:  50 lr:  0.7054802310718645 weight_scale:  0.04878787878787879 loss:  0.3946854037628038\n",
      "iterations:  60 lr:  0.7054802310718645 weight_scale:  0.04878787878787879 loss:  0.37680511881875395\n",
      "iterations:  70 lr:  0.7054802310718645 weight_scale:  0.04878787878787879 loss:  0.36028145697939906\n",
      "iterations:  80 lr:  0.7054802310718645 weight_scale:  0.04878787878787879 loss:  0.3430795697168362\n",
      "iterations:  90 lr:  0.7054802310718645 weight_scale:  0.04878787878787879 loss:  0.3281362846744061\n",
      "iterations:  0 lr:  0.8902150854450392 weight_scale:  0.049595959595959596 loss:  2.343007668557659\n",
      "iterations:  10 lr:  0.8902150854450392 weight_scale:  0.049595959595959596 loss:  0.47984604848935075\n",
      "iterations:  20 lr:  0.8902150854450392 weight_scale:  0.049595959595959596 loss:  0.41665376060624804\n",
      "iterations:  30 lr:  0.8902150854450392 weight_scale:  0.049595959595959596 loss:  0.3841654970318112\n",
      "iterations:  40 lr:  0.8902150854450392 weight_scale:  0.049595959595959596 loss:  0.3583702120597863\n",
      "iterations:  50 lr:  0.8902150854450392 weight_scale:  0.049595959595959596 loss:  0.3353300229457634\n",
      "iterations:  60 lr:  0.8902150854450392 weight_scale:  0.049595959595959596 loss:  0.31496254534055834\n",
      "iterations:  70 lr:  0.8902150854450392 weight_scale:  0.049595959595959596 loss:  0.29712201122924464\n",
      "iterations:  80 lr:  0.8902150854450392 weight_scale:  0.049595959595959596 loss:  0.28057482402000405\n",
      "iterations:  90 lr:  0.8902150854450392 weight_scale:  0.049595959595959596 loss:  0.2594246673237795\n",
      "iterations:  0 lr:  1.1233240329780265 weight_scale:  0.0504040404040404 loss:  2.294519357173862\n",
      "iterations:  10 lr:  1.1233240329780265 weight_scale:  0.0504040404040404 loss:  0.43008795136361977\n",
      "iterations:  20 lr:  1.1233240329780265 weight_scale:  0.0504040404040404 loss:  0.3646665616505842\n",
      "iterations:  30 lr:  1.1233240329780265 weight_scale:  0.0504040404040404 loss:  0.3273074948085038\n",
      "iterations:  40 lr:  1.1233240329780265 weight_scale:  0.0504040404040404 loss:  0.2990276473646368\n",
      "iterations:  50 lr:  1.1233240329780265 weight_scale:  0.0504040404040404 loss:  0.2756745864836041\n",
      "iterations:  60 lr:  1.1233240329780265 weight_scale:  0.0504040404040404 loss:  0.2553795713795766\n",
      "iterations:  70 lr:  1.1233240329780265 weight_scale:  0.0504040404040404 loss:  0.23739847462828753\n",
      "iterations:  80 lr:  1.1233240329780265 weight_scale:  0.0504040404040404 loss:  0.22066422246822462\n",
      "iterations:  90 lr:  1.1233240329780265 weight_scale:  0.0504040404040404 loss:  0.20600492903443754\n",
      "iterations:  0 lr:  1.4174741629268048 weight_scale:  0.051212121212121216 loss:  2.30835401792342\n",
      "iterations:  10 lr:  1.4174741629268048 weight_scale:  0.051212121212121216 loss:  0.39745076628590936\n",
      "iterations:  20 lr:  1.4174741629268048 weight_scale:  0.051212121212121216 loss:  0.32439270491565025\n",
      "iterations:  30 lr:  1.4174741629268048 weight_scale:  0.051212121212121216 loss:  0.2847488846900197\n",
      "iterations:  40 lr:  1.4174741629268048 weight_scale:  0.051212121212121216 loss:  0.25358332374293746\n",
      "iterations:  50 lr:  1.4174741629268048 weight_scale:  0.051212121212121216 loss:  0.22707089105001632\n",
      "iterations:  60 lr:  1.4174741629268048 weight_scale:  0.051212121212121216 loss:  0.2052533380344169\n",
      "iterations:  70 lr:  1.4174741629268048 weight_scale:  0.051212121212121216 loss:  0.18727917793190144\n",
      "iterations:  80 lr:  1.4174741629268048 weight_scale:  0.051212121212121216 loss:  0.1715756147208688\n",
      "iterations:  90 lr:  1.4174741629268048 weight_scale:  0.051212121212121216 loss:  0.15807886578350497\n",
      "iterations:  0 lr:  1.7886495290574351 weight_scale:  0.05202020202020202 loss:  2.3794979699162813\n",
      "iterations:  10 lr:  1.7886495290574351 weight_scale:  0.05202020202020202 loss:  0.3764270388667052\n",
      "iterations:  20 lr:  1.7886495290574351 weight_scale:  0.05202020202020202 loss:  0.3025684612455677\n",
      "iterations:  30 lr:  1.7886495290574351 weight_scale:  0.05202020202020202 loss:  0.2615329403447801\n",
      "iterations:  40 lr:  1.7886495290574351 weight_scale:  0.05202020202020202 loss:  0.2291253658415741\n",
      "iterations:  50 lr:  1.7886495290574351 weight_scale:  0.05202020202020202 loss:  0.2023732989894055\n",
      "iterations:  60 lr:  1.7886495290574351 weight_scale:  0.05202020202020202 loss:  0.1810800986521423\n",
      "iterations:  70 lr:  1.7886495290574351 weight_scale:  0.05202020202020202 loss:  0.16427086858630077\n",
      "iterations:  80 lr:  1.7886495290574351 weight_scale:  0.05202020202020202 loss:  0.15105658355292342\n",
      "iterations:  90 lr:  1.7886495290574351 weight_scale:  0.05202020202020202 loss:  0.13991724006152353\n",
      "iterations:  0 lr:  2.2570197196339215 weight_scale:  0.05282828282828283 loss:  2.43915495257895\n",
      "iterations:  10 lr:  2.2570197196339215 weight_scale:  0.05282828282828283 loss:  0.3097692350509097\n",
      "iterations:  20 lr:  2.2570197196339215 weight_scale:  0.05282828282828283 loss:  0.23808240534073433\n",
      "iterations:  30 lr:  2.2570197196339215 weight_scale:  0.05282828282828283 loss:  0.19928933885983155\n",
      "iterations:  40 lr:  2.2570197196339215 weight_scale:  0.05282828282828283 loss:  0.17135136896152928\n",
      "iterations:  50 lr:  2.2570197196339215 weight_scale:  0.05282828282828283 loss:  0.15008020161577892\n",
      "iterations:  60 lr:  2.2570197196339215 weight_scale:  0.05282828282828283 loss:  0.13310759608017877\n",
      "iterations:  70 lr:  2.2570197196339215 weight_scale:  0.05282828282828283 loss:  0.11993581741043145\n",
      "iterations:  80 lr:  2.2570197196339215 weight_scale:  0.05282828282828283 loss:  0.10858274609833708\n",
      "iterations:  90 lr:  2.2570197196339215 weight_scale:  0.05282828282828283 loss:  0.09899008152731299\n",
      "iterations:  0 lr:  2.848035868435799 weight_scale:  0.05363636363636364 loss:  2.3342702378650824\n",
      "iterations:  10 lr:  2.848035868435799 weight_scale:  0.05363636363636364 loss:  0.32412350106920484\n",
      "iterations:  20 lr:  2.848035868435799 weight_scale:  0.05363636363636364 loss:  0.22976874542392128\n",
      "iterations:  30 lr:  2.848035868435799 weight_scale:  0.05363636363636364 loss:  0.18306901663305328\n",
      "iterations:  40 lr:  2.848035868435799 weight_scale:  0.05363636363636364 loss:  0.15076100742424375\n",
      "iterations:  50 lr:  2.848035868435799 weight_scale:  0.05363636363636364 loss:  0.12888305498186428\n",
      "iterations:  60 lr:  2.848035868435799 weight_scale:  0.05363636363636364 loss:  0.11339987835952468\n",
      "iterations:  70 lr:  2.848035868435799 weight_scale:  0.05363636363636364 loss:  0.10227680454427547\n",
      "iterations:  80 lr:  2.848035868435799 weight_scale:  0.05363636363636364 loss:  0.09357516748758403\n",
      "iterations:  90 lr:  2.848035868435799 weight_scale:  0.05363636363636364 loss:  0.08705434639535142\n",
      "iterations:  0 lr:  3.593813663804626 weight_scale:  0.05444444444444445 loss:  2.374873082031297\n",
      "iterations:  10 lr:  3.593813663804626 weight_scale:  0.05444444444444445 loss:  0.3478439684353872\n",
      "iterations:  20 lr:  3.593813663804626 weight_scale:  0.05444444444444445 loss:  0.24392840236448213\n",
      "iterations:  30 lr:  3.593813663804626 weight_scale:  0.05444444444444445 loss:  0.19282296075983943\n",
      "iterations:  40 lr:  3.593813663804626 weight_scale:  0.05444444444444445 loss:  0.16064541636900007\n",
      "iterations:  50 lr:  3.593813663804626 weight_scale:  0.05444444444444445 loss:  0.14020307497290038\n",
      "iterations:  60 lr:  3.593813663804626 weight_scale:  0.05444444444444445 loss:  0.12617059294177596\n",
      "iterations:  70 lr:  3.593813663804626 weight_scale:  0.05444444444444445 loss:  0.11571673964311019\n",
      "iterations:  80 lr:  3.593813663804626 weight_scale:  0.05444444444444445 loss:  0.10786943038152456\n",
      "iterations:  90 lr:  3.593813663804626 weight_scale:  0.05444444444444445 loss:  0.10166961392866838\n",
      "iterations:  0 lr:  4.534878508128582 weight_scale:  0.055252525252525254 loss:  2.4525245721610562\n",
      "iterations:  10 lr:  4.534878508128582 weight_scale:  0.055252525252525254 loss:  0.28741182370919793\n",
      "iterations:  20 lr:  4.534878508128582 weight_scale:  0.055252525252525254 loss:  0.1841191074169662\n",
      "iterations:  30 lr:  4.534878508128582 weight_scale:  0.055252525252525254 loss:  0.14048090941169883\n",
      "iterations:  40 lr:  4.534878508128582 weight_scale:  0.055252525252525254 loss:  0.11674741463665766\n",
      "iterations:  50 lr:  4.534878508128582 weight_scale:  0.055252525252525254 loss:  0.10194826572098116\n",
      "iterations:  60 lr:  4.534878508128582 weight_scale:  0.055252525252525254 loss:  0.09187825144885263\n",
      "iterations:  70 lr:  4.534878508128582 weight_scale:  0.055252525252525254 loss:  0.08446748893168861\n",
      "iterations:  80 lr:  4.534878508128582 weight_scale:  0.055252525252525254 loss:  0.07868528565714367\n",
      "iterations:  90 lr:  4.534878508128582 weight_scale:  0.055252525252525254 loss:  0.0743201364226257\n",
      "iterations:  0 lr:  5.72236765935022 weight_scale:  0.05606060606060606 loss:  2.3732915532027303\n",
      "iterations:  10 lr:  5.72236765935022 weight_scale:  0.05606060606060606 loss:  0.2664343003793797\n",
      "iterations:  20 lr:  5.72236765935022 weight_scale:  0.05606060606060606 loss:  0.15426440841672873\n",
      "iterations:  30 lr:  5.72236765935022 weight_scale:  0.05606060606060606 loss:  0.11574064776903004\n",
      "iterations:  40 lr:  5.72236765935022 weight_scale:  0.05606060606060606 loss:  0.09478761390837454\n",
      "iterations:  50 lr:  5.72236765935022 weight_scale:  0.05606060606060606 loss:  0.08260590617083315\n",
      "iterations:  60 lr:  5.72236765935022 weight_scale:  0.05606060606060606 loss:  0.0746066249185662\n",
      "iterations:  70 lr:  5.72236765935022 weight_scale:  0.05606060606060606 loss:  0.06888087976109589\n",
      "iterations:  80 lr:  5.72236765935022 weight_scale:  0.05606060606060606 loss:  0.06478058580869854\n",
      "iterations:  90 lr:  5.72236765935022 weight_scale:  0.05606060606060606 loss:  0.061670429571221305\n",
      "iterations:  0 lr:  7.220809018385457 weight_scale:  0.05686868686868687 loss:  2.4209206500154496\n",
      "iterations:  10 lr:  7.220809018385457 weight_scale:  0.05686868686868687 loss:  0.20041676428517463\n",
      "iterations:  20 lr:  7.220809018385457 weight_scale:  0.05686868686868687 loss:  0.11451277331397908\n",
      "iterations:  30 lr:  7.220809018385457 weight_scale:  0.05686868686868687 loss:  0.08506147149374993\n",
      "iterations:  40 lr:  7.220809018385457 weight_scale:  0.05686868686868687 loss:  0.07016134424790176\n",
      "iterations:  50 lr:  7.220809018385457 weight_scale:  0.05686868686868687 loss:  0.061199728108750594\n",
      "iterations:  60 lr:  7.220809018385457 weight_scale:  0.05686868686868687 loss:  0.055198931795970324\n",
      "iterations:  70 lr:  7.220809018385457 weight_scale:  0.05686868686868687 loss:  0.05092126715032453\n",
      "iterations:  80 lr:  7.220809018385457 weight_scale:  0.05686868686868687 loss:  0.047736593240802046\n",
      "iterations:  90 lr:  7.220809018385457 weight_scale:  0.05686868686868687 loss:  0.04519141107091593\n",
      "iterations:  0 lr:  9.111627561154886 weight_scale:  0.05767676767676768 loss:  2.359205880379227\n",
      "iterations:  10 lr:  9.111627561154886 weight_scale:  0.05767676767676768 loss:  0.18337111573847767\n",
      "iterations:  20 lr:  9.111627561154886 weight_scale:  0.05767676767676768 loss:  0.10127015224032773\n",
      "iterations:  30 lr:  9.111627561154886 weight_scale:  0.05767676767676768 loss:  0.07566008057455208\n",
      "iterations:  40 lr:  9.111627561154886 weight_scale:  0.05767676767676768 loss:  0.0634107892782115\n",
      "iterations:  50 lr:  9.111627561154886 weight_scale:  0.05767676767676768 loss:  0.05600604704648061\n",
      "iterations:  60 lr:  9.111627561154886 weight_scale:  0.05767676767676768 loss:  0.05106045996431683\n",
      "iterations:  70 lr:  9.111627561154886 weight_scale:  0.05767676767676768 loss:  0.04747088229891587\n",
      "iterations:  80 lr:  9.111627561154886 weight_scale:  0.05767676767676768 loss:  0.044810123315577384\n",
      "iterations:  90 lr:  9.111627561154886 weight_scale:  0.05767676767676768 loss:  0.042719883774912315\n",
      "iterations:  0 lr:  11.497569953977356 weight_scale:  0.05848484848484849 loss:  2.3198971121884373\n",
      "iterations:  10 lr:  11.497569953977356 weight_scale:  0.05848484848484849 loss:  0.17494820705006797\n",
      "iterations:  20 lr:  11.497569953977356 weight_scale:  0.05848484848484849 loss:  0.09167866516046941\n",
      "iterations:  30 lr:  11.497569953977356 weight_scale:  0.05848484848484849 loss:  0.06774243651246134\n",
      "iterations:  40 lr:  11.497569953977356 weight_scale:  0.05848484848484849 loss:  0.056950804527843964\n",
      "iterations:  50 lr:  11.497569953977356 weight_scale:  0.05848484848484849 loss:  0.05081639041274661\n",
      "iterations:  60 lr:  11.497569953977356 weight_scale:  0.05848484848484849 loss:  0.04680393351626719\n",
      "iterations:  70 lr:  11.497569953977356 weight_scale:  0.05848484848484849 loss:  0.043941314406938915\n",
      "iterations:  80 lr:  11.497569953977356 weight_scale:  0.05848484848484849 loss:  0.041744960428008016\n",
      "iterations:  90 lr:  11.497569953977356 weight_scale:  0.05848484848484849 loss:  0.040074285627653576\n",
      "iterations:  0 lr:  14.508287784959402 weight_scale:  0.05929292929292929 loss:  2.3801589001611614\n",
      "iterations:  10 lr:  14.508287784959402 weight_scale:  0.05929292929292929 loss:  0.14052530249972886\n",
      "iterations:  20 lr:  14.508287784959402 weight_scale:  0.05929292929292929 loss:  0.06985302597147273\n",
      "iterations:  30 lr:  14.508287784959402 weight_scale:  0.05929292929292929 loss:  0.05067624799647664\n",
      "iterations:  40 lr:  14.508287784959402 weight_scale:  0.05929292929292929 loss:  0.04187563509487366\n",
      "iterations:  50 lr:  14.508287784959402 weight_scale:  0.05929292929292929 loss:  0.03679271878252184\n",
      "iterations:  60 lr:  14.508287784959402 weight_scale:  0.05929292929292929 loss:  0.03348069535922206\n",
      "iterations:  70 lr:  14.508287784959402 weight_scale:  0.05929292929292929 loss:  0.03116795202900137\n",
      "iterations:  80 lr:  14.508287784959402 weight_scale:  0.05929292929292929 loss:  0.029460054774375247\n",
      "iterations:  90 lr:  14.508287784959402 weight_scale:  0.05929292929292929 loss:  0.02814990835560151\n",
      "iterations:  0 lr:  18.30738280295366 weight_scale:  0.060101010101010106 loss:  2.3100663312029597\n",
      "iterations:  10 lr:  18.30738280295366 weight_scale:  0.060101010101010106 loss:  0.13722947870039381\n",
      "iterations:  20 lr:  18.30738280295366 weight_scale:  0.060101010101010106 loss:  0.07363591014887914\n",
      "iterations:  30 lr:  18.30738280295366 weight_scale:  0.060101010101010106 loss:  0.05568470726315339\n",
      "iterations:  40 lr:  18.30738280295366 weight_scale:  0.060101010101010106 loss:  0.04671951654979952\n",
      "iterations:  50 lr:  18.30738280295366 weight_scale:  0.060101010101010106 loss:  0.04154895684065227\n",
      "iterations:  60 lr:  18.30738280295366 weight_scale:  0.060101010101010106 loss:  0.038189404403780214\n",
      "iterations:  70 lr:  18.30738280295366 weight_scale:  0.060101010101010106 loss:  0.0358682128777164\n",
      "iterations:  80 lr:  18.30738280295366 weight_scale:  0.060101010101010106 loss:  0.03414450846529375\n",
      "iterations:  90 lr:  18.30738280295366 weight_scale:  0.060101010101010106 loss:  0.03280186068683586\n",
      "iterations:  0 lr:  23.10129700083158 weight_scale:  0.06090909090909091 loss:  2.3783256989023536\n",
      "iterations:  10 lr:  23.10129700083158 weight_scale:  0.06090909090909091 loss:  0.11672782789063145\n",
      "iterations:  20 lr:  23.10129700083158 weight_scale:  0.06090909090909091 loss:  0.06275395896984177\n",
      "iterations:  30 lr:  23.10129700083158 weight_scale:  0.06090909090909091 loss:  0.04769286344138265\n",
      "iterations:  40 lr:  23.10129700083158 weight_scale:  0.06090909090909091 loss:  0.04054409593787445\n",
      "iterations:  50 lr:  23.10129700083158 weight_scale:  0.06090909090909091 loss:  0.03630337827875045\n",
      "iterations:  60 lr:  23.10129700083158 weight_scale:  0.06090909090909091 loss:  0.03361188829669887\n",
      "iterations:  70 lr:  23.10129700083158 weight_scale:  0.06090909090909091 loss:  0.03172025776323689\n",
      "iterations:  80 lr:  23.10129700083158 weight_scale:  0.06090909090909091 loss:  0.030323719127309193\n",
      "iterations:  90 lr:  23.10129700083158 weight_scale:  0.06090909090909091 loss:  0.029253878497362006\n",
      "iterations:  0 lr:  29.150530628251758 weight_scale:  0.06171717171717172 loss:  2.3494692198547464\n",
      "iterations:  10 lr:  29.150530628251758 weight_scale:  0.06171717171717172 loss:  0.10270298361337822\n",
      "iterations:  20 lr:  29.150530628251758 weight_scale:  0.06171717171717172 loss:  0.05159722167412057\n",
      "iterations:  30 lr:  29.150530628251758 weight_scale:  0.06171717171717172 loss:  0.038509226134451666\n",
      "iterations:  40 lr:  29.150530628251758 weight_scale:  0.06171717171717172 loss:  0.032351766337926635\n",
      "iterations:  50 lr:  29.150530628251758 weight_scale:  0.06171717171717172 loss:  0.02880255102372804\n",
      "iterations:  60 lr:  29.150530628251758 weight_scale:  0.06171717171717172 loss:  0.026523600832809215\n",
      "iterations:  70 lr:  29.150530628251758 weight_scale:  0.06171717171717172 loss:  0.024933093303206113\n",
      "iterations:  80 lr:  29.150530628251758 weight_scale:  0.06171717171717172 loss:  0.023756195874269123\n",
      "iterations:  90 lr:  29.150530628251758 weight_scale:  0.06171717171717172 loss:  0.02286000496269456\n",
      "iterations:  0 lr:  36.783797718286344 weight_scale:  0.06252525252525253 loss:  2.3958230292273406\n",
      "iterations:  10 lr:  36.783797718286344 weight_scale:  0.06252525252525253 loss:  0.09710943019932135\n",
      "iterations:  20 lr:  36.783797718286344 weight_scale:  0.06252525252525253 loss:  0.04646757796931955\n",
      "iterations:  30 lr:  36.783797718286344 weight_scale:  0.06252525252525253 loss:  0.0342209938456977\n",
      "iterations:  40 lr:  36.783797718286344 weight_scale:  0.06252525252525253 loss:  0.02843774684563733\n",
      "iterations:  50 lr:  36.783797718286344 weight_scale:  0.06252525252525253 loss:  0.02511885189735635\n",
      "iterations:  60 lr:  36.783797718286344 weight_scale:  0.06252525252525253 loss:  0.02296184996398355\n",
      "iterations:  70 lr:  36.783797718286344 weight_scale:  0.06252525252525253 loss:  0.021442672055525757\n",
      "iterations:  80 lr:  36.783797718286344 weight_scale:  0.06252525252525253 loss:  0.020277796114151418\n",
      "iterations:  90 lr:  36.783797718286344 weight_scale:  0.06252525252525253 loss:  0.01937384647638292\n",
      "iterations:  0 lr:  46.41588833612782 weight_scale:  0.06333333333333332 loss:  2.448533724611321\n",
      "iterations:  10 lr:  46.41588833612782 weight_scale:  0.06333333333333332 loss:  0.09772915984291357\n",
      "iterations:  20 lr:  46.41588833612782 weight_scale:  0.06333333333333332 loss:  0.050569461879386345\n",
      "iterations:  30 lr:  46.41588833612782 weight_scale:  0.06333333333333332 loss:  0.03788098321607564\n",
      "iterations:  40 lr:  46.41588833612782 weight_scale:  0.06333333333333332 loss:  0.031927608437167014\n",
      "iterations:  50 lr:  46.41588833612782 weight_scale:  0.06333333333333332 loss:  0.028565739543094954\n",
      "iterations:  60 lr:  46.41588833612782 weight_scale:  0.06333333333333332 loss:  0.026314678281466258\n",
      "iterations:  70 lr:  46.41588833612782 weight_scale:  0.06333333333333332 loss:  0.024658658511266553\n",
      "iterations:  80 lr:  46.41588833612782 weight_scale:  0.06333333333333332 loss:  0.023366133023420405\n",
      "iterations:  90 lr:  46.41588833612782 weight_scale:  0.06333333333333332 loss:  0.022343004796605243\n",
      "iterations:  0 lr:  58.57020818056661 weight_scale:  0.06414141414141414 loss:  2.355615358185491\n",
      "iterations:  10 lr:  58.57020818056661 weight_scale:  0.06414141414141414 loss:  0.0937052034320597\n",
      "iterations:  20 lr:  58.57020818056661 weight_scale:  0.06414141414141414 loss:  0.045209873828236276\n",
      "iterations:  30 lr:  58.57020818056661 weight_scale:  0.06414141414141414 loss:  0.03308883442987789\n",
      "iterations:  40 lr:  58.57020818056661 weight_scale:  0.06414141414141414 loss:  0.027498553838781414\n",
      "iterations:  50 lr:  58.57020818056661 weight_scale:  0.06414141414141414 loss:  0.024133387868345815\n",
      "iterations:  60 lr:  58.57020818056661 weight_scale:  0.06414141414141414 loss:  0.021945599028804774\n",
      "iterations:  70 lr:  58.57020818056661 weight_scale:  0.06414141414141414 loss:  0.02038829429381131\n",
      "iterations:  80 lr:  58.57020818056661 weight_scale:  0.06414141414141414 loss:  0.019216343937465342\n",
      "iterations:  90 lr:  58.57020818056661 weight_scale:  0.06414141414141414 loss:  0.01828416018834804\n",
      "iterations:  0 lr:  73.90722033525775 weight_scale:  0.06494949494949495 loss:  2.404396992213339\n",
      "iterations:  10 lr:  73.90722033525775 weight_scale:  0.06494949494949495 loss:  0.08222672855535801\n",
      "iterations:  20 lr:  73.90722033525775 weight_scale:  0.06494949494949495 loss:  0.0444529216896135\n",
      "iterations:  30 lr:  73.90722033525775 weight_scale:  0.06494949494949495 loss:  0.034039650110530485\n",
      "iterations:  40 lr:  73.90722033525775 weight_scale:  0.06494949494949495 loss:  0.028973232546800173\n",
      "iterations:  50 lr:  73.90722033525775 weight_scale:  0.06494949494949495 loss:  0.025907876035849493\n",
      "iterations:  60 lr:  73.90722033525775 weight_scale:  0.06494949494949495 loss:  0.023789470645801246\n",
      "iterations:  70 lr:  73.90722033525775 weight_scale:  0.06494949494949495 loss:  0.022287931227883258\n",
      "iterations:  80 lr:  73.90722033525775 weight_scale:  0.06494949494949495 loss:  0.02108707633566596\n",
      "iterations:  90 lr:  73.90722033525775 weight_scale:  0.06494949494949495 loss:  0.020113266990654473\n",
      "iterations:  0 lr:  93.26033468832199 weight_scale:  0.06575757575757575 loss:  2.356190330075062\n",
      "iterations:  10 lr:  93.26033468832199 weight_scale:  0.06575757575757575 loss:  0.08704675227800013\n",
      "iterations:  20 lr:  93.26033468832199 weight_scale:  0.06575757575757575 loss:  0.043523197745089705\n",
      "iterations:  30 lr:  93.26033468832199 weight_scale:  0.06575757575757575 loss:  0.03353985644103437\n",
      "iterations:  40 lr:  93.26033468832199 weight_scale:  0.06575757575757575 loss:  0.02925100390414154\n",
      "iterations:  50 lr:  93.26033468832199 weight_scale:  0.06575757575757575 loss:  0.026759848531366766\n",
      "iterations:  60 lr:  93.26033468832199 weight_scale:  0.06575757575757575 loss:  0.024848862878575368\n",
      "iterations:  70 lr:  93.26033468832199 weight_scale:  0.06575757575757575 loss:  0.023388120910453555\n",
      "iterations:  80 lr:  93.26033468832199 weight_scale:  0.06575757575757575 loss:  0.022070829598636113\n",
      "iterations:  90 lr:  93.26033468832199 weight_scale:  0.06575757575757575 loss:  0.021003097775311425\n",
      "iterations:  0 lr:  117.68119524349991 weight_scale:  0.06656565656565656 loss:  2.3697138201568113\n",
      "iterations:  10 lr:  117.68119524349991 weight_scale:  0.06656565656565656 loss:  0.3417474385686323\n",
      "iterations:  20 lr:  117.68119524349991 weight_scale:  0.06656565656565656 loss:  0.06464080622237571\n",
      "iterations:  30 lr:  117.68119524349991 weight_scale:  0.06656565656565656 loss:  0.049977778494173154\n",
      "iterations:  40 lr:  117.68119524349991 weight_scale:  0.06656565656565656 loss:  0.042974375033383944\n",
      "iterations:  50 lr:  117.68119524349991 weight_scale:  0.06656565656565656 loss:  0.03842645986305963\n",
      "iterations:  60 lr:  117.68119524349991 weight_scale:  0.06656565656565656 loss:  0.035607546169731635\n",
      "iterations:  70 lr:  117.68119524349991 weight_scale:  0.06656565656565656 loss:  0.03351331906232644\n",
      "iterations:  80 lr:  117.68119524349991 weight_scale:  0.06656565656565656 loss:  0.03157690240184745\n",
      "iterations:  90 lr:  117.68119524349991 weight_scale:  0.06656565656565656 loss:  0.030041523127602207\n",
      "iterations:  0 lr:  148.49682622544634 weight_scale:  0.06737373737373738 loss:  2.3564124654981464\n",
      "iterations:  10 lr:  148.49682622544634 weight_scale:  0.06737373737373738 loss:  0.8146699245675896\n",
      "iterations:  20 lr:  148.49682622544634 weight_scale:  0.06737373737373738 loss:  1.0566282706761647\n",
      "iterations:  30 lr:  148.49682622544634 weight_scale:  0.06737373737373738 loss:  0.8412804563836902\n",
      "iterations:  40 lr:  148.49682622544634 weight_scale:  0.06737373737373738 loss:  0.579857811723868\n",
      "iterations:  50 lr:  148.49682622544634 weight_scale:  0.06737373737373738 loss:  0.5890571587714731\n",
      "iterations:  60 lr:  148.49682622544634 weight_scale:  0.06737373737373738 loss:  0.7315842164671419\n",
      "iterations:  70 lr:  148.49682622544634 weight_scale:  0.06737373737373738 loss:  0.30917041741588913\n",
      "iterations:  80 lr:  148.49682622544634 weight_scale:  0.06737373737373738 loss:  0.25501583582064424\n",
      "iterations:  90 lr:  148.49682622544634 weight_scale:  0.06737373737373738 loss:  0.24922443171517308\n",
      "iterations:  0 lr:  187.3817422860383 weight_scale:  0.06818181818181818 loss:  2.461716049138679\n",
      "iterations:  10 lr:  187.3817422860383 weight_scale:  0.06818181818181818 loss:  3.872546289772968\n",
      "iterations:  20 lr:  187.3817422860383 weight_scale:  0.06818181818181818 loss:  2.25100993303833\n",
      "iterations:  30 lr:  187.3817422860383 weight_scale:  0.06818181818181818 loss:  1.8189543677319384\n",
      "iterations:  40 lr:  187.3817422860383 weight_scale:  0.06818181818181818 loss:  1.8763470734872565\n",
      "iterations:  50 lr:  187.3817422860383 weight_scale:  0.06818181818181818 loss:  1.5689584026727266\n",
      "iterations:  60 lr:  187.3817422860383 weight_scale:  0.06818181818181818 loss:  1.319356409577663\n",
      "iterations:  70 lr:  187.3817422860383 weight_scale:  0.06818181818181818 loss:  1.5714728102771311\n",
      "iterations:  80 lr:  187.3817422860383 weight_scale:  0.06818181818181818 loss:  1.656945975203783\n",
      "iterations:  90 lr:  187.3817422860383 weight_scale:  0.06818181818181818 loss:  1.39123907087225\n",
      "iterations:  0 lr:  236.4489412645407 weight_scale:  0.06898989898989899 loss:  2.427305812770826\n",
      "iterations:  10 lr:  236.4489412645407 weight_scale:  0.06898989898989899 loss:  2.3411768265028257\n",
      "iterations:  20 lr:  236.4489412645407 weight_scale:  0.06898989898989899 loss:  2.5239553333011533\n",
      "iterations:  30 lr:  236.4489412645407 weight_scale:  0.06898989898989899 loss:  1.9247652992488034\n",
      "iterations:  40 lr:  236.4489412645407 weight_scale:  0.06898989898989899 loss:  2.2804749241926605\n",
      "iterations:  50 lr:  236.4489412645407 weight_scale:  0.06898989898989899 loss:  2.0782061433837438\n",
      "iterations:  60 lr:  236.4489412645407 weight_scale:  0.06898989898989899 loss:  2.1527212942685776\n",
      "iterations:  70 lr:  236.4489412645407 weight_scale:  0.06898989898989899 loss:  2.1139397470932773\n",
      "iterations:  80 lr:  236.4489412645407 weight_scale:  0.06898989898989899 loss:  2.084955433047609\n",
      "iterations:  90 lr:  236.4489412645407 weight_scale:  0.06898989898989899 loss:  2.0236472516862873\n",
      "iterations:  0 lr:  298.364724028334 weight_scale:  0.06979797979797979 loss:  2.4607336363210512\n",
      "iterations:  10 lr:  298.364724028334 weight_scale:  0.06979797979797979 loss:  2.4086276085447382\n",
      "iterations:  20 lr:  298.364724028334 weight_scale:  0.06979797979797979 loss:  2.050222178569233\n",
      "iterations:  30 lr:  298.364724028334 weight_scale:  0.06979797979797979 loss:  2.1036826657817143\n",
      "iterations:  40 lr:  298.364724028334 weight_scale:  0.06979797979797979 loss:  1.9812941408677753\n",
      "iterations:  50 lr:  298.364724028334 weight_scale:  0.06979797979797979 loss:  2.096377404216942\n",
      "iterations:  60 lr:  298.364724028334 weight_scale:  0.06979797979797979 loss:  2.074247245282268\n",
      "iterations:  70 lr:  298.364724028334 weight_scale:  0.06979797979797979 loss:  1.9497149856101288\n",
      "iterations:  80 lr:  298.364724028334 weight_scale:  0.06979797979797979 loss:  1.9335915183695633\n",
      "iterations:  90 lr:  298.364724028334 weight_scale:  0.06979797979797979 loss:  1.9186952283939795\n",
      "iterations:  0 lr:  376.49358067924635 weight_scale:  0.0706060606060606 loss:  2.466936206404111\n",
      "iterations:  10 lr:  376.49358067924635 weight_scale:  0.0706060606060606 loss:  2.038651930519458\n",
      "iterations:  20 lr:  376.49358067924635 weight_scale:  0.0706060606060606 loss:  2.0993178072399328\n",
      "iterations:  30 lr:  376.49358067924635 weight_scale:  0.0706060606060606 loss:  1.976437155096684\n",
      "iterations:  40 lr:  376.49358067924635 weight_scale:  0.0706060606060606 loss:  2.0092028616897624\n",
      "iterations:  50 lr:  376.49358067924635 weight_scale:  0.0706060606060606 loss:  2.0164128192835635\n",
      "iterations:  60 lr:  376.49358067924635 weight_scale:  0.0706060606060606 loss:  2.0346265550014455\n",
      "iterations:  70 lr:  376.49358067924635 weight_scale:  0.0706060606060606 loss:  2.0230611046648197\n",
      "iterations:  80 lr:  376.49358067924635 weight_scale:  0.0706060606060606 loss:  2.058149881171944\n",
      "iterations:  90 lr:  376.49358067924635 weight_scale:  0.0706060606060606 loss:  1.9997239716681334\n",
      "iterations:  0 lr:  475.0810162102793 weight_scale:  0.07141414141414142 loss:  2.4711624031152373\n",
      "iterations:  10 lr:  475.0810162102793 weight_scale:  0.07141414141414142 loss:  2.2724746332936396\n",
      "iterations:  20 lr:  475.0810162102793 weight_scale:  0.07141414141414142 loss:  2.2377426567760064\n",
      "iterations:  30 lr:  475.0810162102793 weight_scale:  0.07141414141414142 loss:  2.217236169247359\n",
      "iterations:  40 lr:  475.0810162102793 weight_scale:  0.07141414141414142 loss:  2.198065718692229\n",
      "iterations:  50 lr:  475.0810162102793 weight_scale:  0.07141414141414142 loss:  2.1946792642833275\n",
      "iterations:  60 lr:  475.0810162102793 weight_scale:  0.07141414141414142 loss:  2.1732784576909006\n",
      "iterations:  70 lr:  475.0810162102793 weight_scale:  0.07141414141414142 loss:  2.193164083467685\n",
      "iterations:  80 lr:  475.0810162102793 weight_scale:  0.07141414141414142 loss:  2.2063766656963324\n",
      "iterations:  90 lr:  475.0810162102793 weight_scale:  0.07141414141414142 loss:  2.2029520776494893\n",
      "iterations:  0 lr:  599.4842503189409 weight_scale:  0.07222222222222222 loss:  2.421899621949628\n",
      "iterations:  10 lr:  599.4842503189409 weight_scale:  0.07222222222222222 loss:  2.2866779372769015\n",
      "iterations:  20 lr:  599.4842503189409 weight_scale:  0.07222222222222222 loss:  2.2830644993683302\n",
      "iterations:  30 lr:  599.4842503189409 weight_scale:  0.07222222222222222 loss:  2.2848966202427836\n",
      "iterations:  40 lr:  599.4842503189409 weight_scale:  0.07222222222222222 loss:  2.307808531878265\n",
      "iterations:  50 lr:  599.4842503189409 weight_scale:  0.07222222222222222 loss:  2.298421235712555\n",
      "iterations:  60 lr:  599.4842503189409 weight_scale:  0.07222222222222222 loss:  2.296641921141104\n",
      "iterations:  70 lr:  599.4842503189409 weight_scale:  0.07222222222222222 loss:  2.2763018644094455\n",
      "iterations:  80 lr:  599.4842503189409 weight_scale:  0.07222222222222222 loss:  2.2754937327452107\n",
      "iterations:  90 lr:  599.4842503189409 weight_scale:  0.07222222222222222 loss:  2.2929356109389296\n",
      "iterations:  0 lr:  756.463327554629 weight_scale:  0.07303030303030303 loss:  2.3103717737163043\n",
      "iterations:  10 lr:  756.463327554629 weight_scale:  0.07303030303030303 loss:  2.3011764948324474\n",
      "iterations:  20 lr:  756.463327554629 weight_scale:  0.07303030303030303 loss:  2.2998416310060357\n",
      "iterations:  30 lr:  756.463327554629 weight_scale:  0.07303030303030303 loss:  2.3036817132133907\n",
      "iterations:  40 lr:  756.463327554629 weight_scale:  0.07303030303030303 loss:  2.3109167944953803\n",
      "iterations:  50 lr:  756.463327554629 weight_scale:  0.07303030303030303 loss:  2.2944548824041937\n",
      "iterations:  60 lr:  756.463327554629 weight_scale:  0.07303030303030303 loss:  2.3160363410256637\n",
      "iterations:  70 lr:  756.463327554629 weight_scale:  0.07303030303030303 loss:  2.3110725851865257\n",
      "iterations:  80 lr:  756.463327554629 weight_scale:  0.07303030303030303 loss:  2.283804465391063\n",
      "iterations:  90 lr:  756.463327554629 weight_scale:  0.07303030303030303 loss:  2.2821855797934867\n",
      "iterations:  0 lr:  954.5484566618328 weight_scale:  0.07383838383838383 loss:  2.4367220662608453\n",
      "iterations:  10 lr:  954.5484566618328 weight_scale:  0.07383838383838383 loss:  2.3025850929940455\n",
      "iterations:  20 lr:  954.5484566618328 weight_scale:  0.07383838383838383 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  954.5484566618328 weight_scale:  0.07383838383838383 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  954.5484566618328 weight_scale:  0.07383838383838383 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  954.5484566618328 weight_scale:  0.07383838383838383 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  954.5484566618328 weight_scale:  0.07383838383838383 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  954.5484566618328 weight_scale:  0.07383838383838383 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  954.5484566618328 weight_scale:  0.07383838383838383 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  954.5484566618328 weight_scale:  0.07383838383838383 loss:  2.3025850929940455\n",
      "iterations:  0 lr:  1204.5035402587837 weight_scale:  0.07464646464646464 loss:  2.4859282432401484\n",
      "iterations:  10 lr:  1204.5035402587837 weight_scale:  0.07464646464646464 loss:  2.3025850929940455\n",
      "iterations:  20 lr:  1204.5035402587837 weight_scale:  0.07464646464646464 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  1204.5035402587837 weight_scale:  0.07464646464646464 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  1204.5035402587837 weight_scale:  0.07464646464646464 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  1204.5035402587837 weight_scale:  0.07464646464646464 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  1204.5035402587837 weight_scale:  0.07464646464646464 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  1204.5035402587837 weight_scale:  0.07464646464646464 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  1204.5035402587837 weight_scale:  0.07464646464646464 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  1204.5035402587837 weight_scale:  0.07464646464646464 loss:  2.3025850929940455\n",
      "iterations:  0 lr:  1519.9110829529332 weight_scale:  0.07545454545454545 loss:  2.411241453331973\n",
      "iterations:  10 lr:  1519.9110829529332 weight_scale:  0.07545454545454545 loss:  2.3025850929940455\n",
      "iterations:  20 lr:  1519.9110829529332 weight_scale:  0.07545454545454545 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  1519.9110829529332 weight_scale:  0.07545454545454545 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  1519.9110829529332 weight_scale:  0.07545454545454545 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  1519.9110829529332 weight_scale:  0.07545454545454545 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  1519.9110829529332 weight_scale:  0.07545454545454545 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  1519.9110829529332 weight_scale:  0.07545454545454545 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  1519.9110829529332 weight_scale:  0.07545454545454545 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  1519.9110829529332 weight_scale:  0.07545454545454545 loss:  2.3025850929940455\n",
      "iterations:  0 lr:  1917.910261672485 weight_scale:  0.07626262626262625 loss:  2.3753746283247184\n",
      "iterations:  10 lr:  1917.910261672485 weight_scale:  0.07626262626262625 loss:  2.3025850929940455\n",
      "iterations:  20 lr:  1917.910261672485 weight_scale:  0.07626262626262625 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  1917.910261672485 weight_scale:  0.07626262626262625 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  1917.910261672485 weight_scale:  0.07626262626262625 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  1917.910261672485 weight_scale:  0.07626262626262625 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  1917.910261672485 weight_scale:  0.07626262626262625 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  1917.910261672485 weight_scale:  0.07626262626262625 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  1917.910261672485 weight_scale:  0.07626262626262625 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  1917.910261672485 weight_scale:  0.07626262626262625 loss:  2.3025850929940455\n",
      "iterations:  0 lr:  2420.1282647943835 weight_scale:  0.07707070707070707 loss:  2.376523153998839\n",
      "iterations:  10 lr:  2420.1282647943835 weight_scale:  0.07707070707070707 loss:  2.3025850929940455\n",
      "iterations:  20 lr:  2420.1282647943835 weight_scale:  0.07707070707070707 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  2420.1282647943835 weight_scale:  0.07707070707070707 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  2420.1282647943835 weight_scale:  0.07707070707070707 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  2420.1282647943835 weight_scale:  0.07707070707070707 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  2420.1282647943835 weight_scale:  0.07707070707070707 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  2420.1282647943835 weight_scale:  0.07707070707070707 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  2420.1282647943835 weight_scale:  0.07707070707070707 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  2420.1282647943835 weight_scale:  0.07707070707070707 loss:  2.3025850929940455\n",
      "iterations:  0 lr:  3053.8555088334124 weight_scale:  0.07787878787878787 loss:  2.5119217302299552\n",
      "iterations:  10 lr:  3053.8555088334124 weight_scale:  0.07787878787878787 loss:  2.3025850929940455\n",
      "iterations:  20 lr:  3053.8555088334124 weight_scale:  0.07787878787878787 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  3053.8555088334124 weight_scale:  0.07787878787878787 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  3053.8555088334124 weight_scale:  0.07787878787878787 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  3053.8555088334124 weight_scale:  0.07787878787878787 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  3053.8555088334124 weight_scale:  0.07787878787878787 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  3053.8555088334124 weight_scale:  0.07787878787878787 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  3053.8555088334124 weight_scale:  0.07787878787878787 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  3053.8555088334124 weight_scale:  0.07787878787878787 loss:  2.3025850929940455\n",
      "iterations:  0 lr:  3853.5285937105355 weight_scale:  0.07868686868686868 loss:  2.3669562097361796\n",
      "iterations:  10 lr:  3853.5285937105355 weight_scale:  0.07868686868686868 loss:  2.3025850929940455\n",
      "iterations:  20 lr:  3853.5285937105355 weight_scale:  0.07868686868686868 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  3853.5285937105355 weight_scale:  0.07868686868686868 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  3853.5285937105355 weight_scale:  0.07868686868686868 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  3853.5285937105355 weight_scale:  0.07868686868686868 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  3853.5285937105355 weight_scale:  0.07868686868686868 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  3853.5285937105355 weight_scale:  0.07868686868686868 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  3853.5285937105355 weight_scale:  0.07868686868686868 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  3853.5285937105355 weight_scale:  0.07868686868686868 loss:  2.3025850929940455\n",
      "iterations:  0 lr:  4862.601580065353 weight_scale:  0.0794949494949495 loss:  2.3659057252727664\n",
      "iterations:  10 lr:  4862.601580065353 weight_scale:  0.0794949494949495 loss:  2.3025850929940455\n",
      "iterations:  20 lr:  4862.601580065353 weight_scale:  0.0794949494949495 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  4862.601580065353 weight_scale:  0.0794949494949495 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  4862.601580065353 weight_scale:  0.0794949494949495 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  4862.601580065353 weight_scale:  0.0794949494949495 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  4862.601580065353 weight_scale:  0.0794949494949495 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  4862.601580065353 weight_scale:  0.0794949494949495 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  4862.601580065353 weight_scale:  0.0794949494949495 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  4862.601580065353 weight_scale:  0.0794949494949495 loss:  2.3025850929940455\n",
      "iterations:  0 lr:  6135.907273413163 weight_scale:  0.08030303030303029 loss:  2.480300660998504\n",
      "iterations:  10 lr:  6135.907273413163 weight_scale:  0.08030303030303029 loss:  2.3025850929940455\n",
      "iterations:  20 lr:  6135.907273413163 weight_scale:  0.08030303030303029 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  6135.907273413163 weight_scale:  0.08030303030303029 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  6135.907273413163 weight_scale:  0.08030303030303029 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  6135.907273413163 weight_scale:  0.08030303030303029 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  6135.907273413163 weight_scale:  0.08030303030303029 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  6135.907273413163 weight_scale:  0.08030303030303029 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  6135.907273413163 weight_scale:  0.08030303030303029 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  6135.907273413163 weight_scale:  0.08030303030303029 loss:  2.3025850929940455\n",
      "iterations:  0 lr:  7742.636826811277 weight_scale:  0.0811111111111111 loss:  2.511291907212267\n",
      "iterations:  10 lr:  7742.636826811277 weight_scale:  0.0811111111111111 loss:  2.3025850929940455\n",
      "iterations:  20 lr:  7742.636826811277 weight_scale:  0.0811111111111111 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  7742.636826811277 weight_scale:  0.0811111111111111 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  7742.636826811277 weight_scale:  0.0811111111111111 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  7742.636826811277 weight_scale:  0.0811111111111111 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  7742.636826811277 weight_scale:  0.0811111111111111 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  7742.636826811277 weight_scale:  0.0811111111111111 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  7742.636826811277 weight_scale:  0.0811111111111111 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  7742.636826811277 weight_scale:  0.0811111111111111 loss:  2.3025850929940455\n",
      "iterations:  0 lr:  9770.099572992247 weight_scale:  0.08191919191919192 loss:  2.5608211866008643\n",
      "iterations:  10 lr:  9770.099572992247 weight_scale:  0.08191919191919192 loss:  2.3025850929940455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5y/b0xs89p96wg8lzcfm69hdsm40000gn/T/ipykernel_2274/2789001270.py:73: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = np.mean(-np.log(np.sum(softmax * correct_scores, axis=1, keepdims=True)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations:  20 lr:  9770.099572992247 weight_scale:  0.08191919191919192 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  9770.099572992247 weight_scale:  0.08191919191919192 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  9770.099572992247 weight_scale:  0.08191919191919192 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  9770.099572992247 weight_scale:  0.08191919191919192 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  9770.099572992247 weight_scale:  0.08191919191919192 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  9770.099572992247 weight_scale:  0.08191919191919192 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  9770.099572992247 weight_scale:  0.08191919191919192 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  9770.099572992247 weight_scale:  0.08191919191919192 loss:  2.3025850929940455\n",
      "iterations:  0 lr:  12328.467394420633 weight_scale:  0.08272727272727272 loss:  2.290819354575802\n",
      "iterations:  10 lr:  12328.467394420633 weight_scale:  0.08272727272727272 loss:  2.3025850929940455\n",
      "iterations:  20 lr:  12328.467394420633 weight_scale:  0.08272727272727272 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  12328.467394420633 weight_scale:  0.08272727272727272 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  12328.467394420633 weight_scale:  0.08272727272727272 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  12328.467394420633 weight_scale:  0.08272727272727272 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  12328.467394420633 weight_scale:  0.08272727272727272 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  12328.467394420633 weight_scale:  0.08272727272727272 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  12328.467394420633 weight_scale:  0.08272727272727272 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  12328.467394420633 weight_scale:  0.08272727272727272 loss:  2.3025850929940455\n",
      "iterations:  0 lr:  15556.761439304722 weight_scale:  0.08353535353535353 loss:  2.6035581623356436\n",
      "iterations:  10 lr:  15556.761439304722 weight_scale:  0.08353535353535353 loss:  2.3025850929940455\n",
      "iterations:  20 lr:  15556.761439304722 weight_scale:  0.08353535353535353 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  15556.761439304722 weight_scale:  0.08353535353535353 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  15556.761439304722 weight_scale:  0.08353535353535353 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  15556.761439304722 weight_scale:  0.08353535353535353 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  15556.761439304722 weight_scale:  0.08353535353535353 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  15556.761439304722 weight_scale:  0.08353535353535353 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  15556.761439304722 weight_scale:  0.08353535353535353 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  15556.761439304722 weight_scale:  0.08353535353535353 loss:  2.3025850929940455\n",
      "iterations:  0 lr:  19630.406500402685 weight_scale:  0.08434343434343435 loss:  2.3487200199802563\n",
      "iterations:  10 lr:  19630.406500402685 weight_scale:  0.08434343434343435 loss:  2.3025850929940455\n",
      "iterations:  20 lr:  19630.406500402685 weight_scale:  0.08434343434343435 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  19630.406500402685 weight_scale:  0.08434343434343435 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  19630.406500402685 weight_scale:  0.08434343434343435 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  19630.406500402685 weight_scale:  0.08434343434343435 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  19630.406500402685 weight_scale:  0.08434343434343435 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  19630.406500402685 weight_scale:  0.08434343434343435 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  19630.406500402685 weight_scale:  0.08434343434343435 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  19630.406500402685 weight_scale:  0.08434343434343435 loss:  2.3025850929940455\n",
      "iterations:  0 lr:  24770.76355991714 weight_scale:  0.08515151515151514 loss:  2.422978557940764\n",
      "iterations:  10 lr:  24770.76355991714 weight_scale:  0.08515151515151514 loss:  2.3025850929940455\n",
      "iterations:  20 lr:  24770.76355991714 weight_scale:  0.08515151515151514 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  24770.76355991714 weight_scale:  0.08515151515151514 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  24770.76355991714 weight_scale:  0.08515151515151514 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  24770.76355991714 weight_scale:  0.08515151515151514 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  24770.76355991714 weight_scale:  0.08515151515151514 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  24770.76355991714 weight_scale:  0.08515151515151514 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  24770.76355991714 weight_scale:  0.08515151515151514 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  24770.76355991714 weight_scale:  0.08515151515151514 loss:  2.3025850929940455\n",
      "iterations:  0 lr:  31257.15849688235 weight_scale:  0.08595959595959596 loss:  2.353525338809016\n",
      "iterations:  10 lr:  31257.15849688235 weight_scale:  0.08595959595959596 loss:  2.3025850929940455\n",
      "iterations:  20 lr:  31257.15849688235 weight_scale:  0.08595959595959596 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  31257.15849688235 weight_scale:  0.08595959595959596 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  31257.15849688235 weight_scale:  0.08595959595959596 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  31257.15849688235 weight_scale:  0.08595959595959596 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  31257.15849688235 weight_scale:  0.08595959595959596 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  31257.15849688235 weight_scale:  0.08595959595959596 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  31257.15849688235 weight_scale:  0.08595959595959596 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  31257.15849688235 weight_scale:  0.08595959595959596 loss:  2.3025850929940455\n",
      "iterations:  0 lr:  39442.06059437648 weight_scale:  0.08676767676767676 loss:  2.4183729378681362\n",
      "iterations:  10 lr:  39442.06059437648 weight_scale:  0.08676767676767676 loss:  2.3025850929940455\n",
      "iterations:  20 lr:  39442.06059437648 weight_scale:  0.08676767676767676 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  39442.06059437648 weight_scale:  0.08676767676767676 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  39442.06059437648 weight_scale:  0.08676767676767676 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  39442.06059437648 weight_scale:  0.08676767676767676 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  39442.06059437648 weight_scale:  0.08676767676767676 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  39442.06059437648 weight_scale:  0.08676767676767676 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  39442.06059437648 weight_scale:  0.08676767676767676 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  39442.06059437648 weight_scale:  0.08676767676767676 loss:  2.3025850929940455\n",
      "iterations:  0 lr:  49770.23564332114 weight_scale:  0.08757575757575757 loss:  2.477949832277388\n",
      "iterations:  10 lr:  49770.23564332114 weight_scale:  0.08757575757575757 loss:  2.3025850929940455\n",
      "iterations:  20 lr:  49770.23564332114 weight_scale:  0.08757575757575757 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  49770.23564332114 weight_scale:  0.08757575757575757 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  49770.23564332114 weight_scale:  0.08757575757575757 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  49770.23564332114 weight_scale:  0.08757575757575757 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  49770.23564332114 weight_scale:  0.08757575757575757 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  49770.23564332114 weight_scale:  0.08757575757575757 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  49770.23564332114 weight_scale:  0.08757575757575757 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  49770.23564332114 weight_scale:  0.08757575757575757 loss:  2.3025850929940455\n",
      "iterations:  0 lr:  62802.914418342465 weight_scale:  0.08838383838383838 loss:  2.538603873889788\n",
      "iterations:  10 lr:  62802.914418342465 weight_scale:  0.08838383838383838 loss:  2.3025850929940455\n",
      "iterations:  20 lr:  62802.914418342465 weight_scale:  0.08838383838383838 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  62802.914418342465 weight_scale:  0.08838383838383838 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  62802.914418342465 weight_scale:  0.08838383838383838 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  62802.914418342465 weight_scale:  0.08838383838383838 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  62802.914418342465 weight_scale:  0.08838383838383838 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  62802.914418342465 weight_scale:  0.08838383838383838 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  62802.914418342465 weight_scale:  0.08838383838383838 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  62802.914418342465 weight_scale:  0.08838383838383838 loss:  2.3025850929940455\n",
      "iterations:  0 lr:  79248.28983539186 weight_scale:  0.08919191919191918 loss:  2.447708785108846\n",
      "iterations:  10 lr:  79248.28983539186 weight_scale:  0.08919191919191918 loss:  2.3025850929940455\n",
      "iterations:  20 lr:  79248.28983539186 weight_scale:  0.08919191919191918 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  79248.28983539186 weight_scale:  0.08919191919191918 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  79248.28983539186 weight_scale:  0.08919191919191918 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  79248.28983539186 weight_scale:  0.08919191919191918 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  79248.28983539186 weight_scale:  0.08919191919191918 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  79248.28983539186 weight_scale:  0.08919191919191918 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  79248.28983539186 weight_scale:  0.08919191919191918 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  79248.28983539186 weight_scale:  0.08919191919191918 loss:  2.3025850929940455\n",
      "iterations:  0 lr:  100000.0 weight_scale:  0.09 loss:  2.538080372402124\n",
      "iterations:  10 lr:  100000.0 weight_scale:  0.09 loss:  2.3025850929940455\n",
      "iterations:  20 lr:  100000.0 weight_scale:  0.09 loss:  2.3025850929940455\n",
      "iterations:  30 lr:  100000.0 weight_scale:  0.09 loss:  2.3025850929940455\n",
      "iterations:  40 lr:  100000.0 weight_scale:  0.09 loss:  2.3025850929940455\n",
      "iterations:  50 lr:  100000.0 weight_scale:  0.09 loss:  2.3025850929940455\n",
      "iterations:  60 lr:  100000.0 weight_scale:  0.09 loss:  2.3025850929940455\n",
      "iterations:  70 lr:  100000.0 weight_scale:  0.09 loss:  2.3025850929940455\n",
      "iterations:  80 lr:  100000.0 weight_scale:  0.09 loss:  2.3025850929940455\n",
      "iterations:  90 lr:  100000.0 weight_scale:  0.09 loss:  2.3025850929940455\n",
      "min loss found: 0.017578190872189377\n",
      "best parameters found: [(58.57020818056661, 0.06414141414141414)]\n"
     ]
    }
   ],
   "source": [
    "# Lets update the weights using the gradients. We will use sgd first. Try different hyperparameters.\n",
    "iterations = 100\n",
    "\n",
    "learning_scale = np.geomspace(1e-5, 1e5, num=100)\n",
    "weights_scale = np.linspace(1e-2, 9e-2, num=100)\n",
    "loss_history = {}\n",
    "\n",
    "for learning_rate, weight_scale in zip(learning_scale, weights_scale):\n",
    "    solver = Solver(X, y, N, D, num_layers, H_dims, learning_rate, weight_scale, reg_strength)\n",
    "    model_params = solver.param_init()\n",
    "    for i in range(iterations):\n",
    "        loss, dL, affine_cache, batch_cache, relu_cache = solver.forward_pass(model_params)\n",
    "        grads = solver.backward_pass(loss, dL, affine_cache, batch_cache, relu_cache)\n",
    "        model_params = solver.sgd(grads, model_params)  # note, since we are not taking batches, its technically batch grad descent.\n",
    "        if i == 0 or i % 10 == 0:\n",
    "            print('iterations: ', i, 'lr: ', learning_rate, 'weight_scale: ', weight_scale, 'loss: ', loss)\n",
    "    loss_history[(learning_rate, weight_scale)] = loss\n",
    "\n",
    "best_params = [params for params, loss in loss_history.items() if loss == min(loss_history.values())]\n",
    "print(f'min loss found: {min(loss_history.values())}')\n",
    "print(f'best parameters found: {best_params}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
